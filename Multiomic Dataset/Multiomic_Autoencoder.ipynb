{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-omic Dataset: Latent representation using Autoencoders\n",
    "- Load Data\n",
    "- Normalize Data\n",
    "- Define Autoencoder Model\n",
    "- Train Autoencoder with normalized dataset\n",
    "- Use transformed dataset for classification\n",
    "- Use transformed dataset for clustering\n",
    "- Evaluation and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./x_exp_renal.csv' does not exist: b'./x_exp_renal.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ba90219f3574>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Load Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mX_renal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./x_exp_renal.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Dataset has Donor ID as first column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0my_renal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./y_renal.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Dataset has Donor ID on first column and Label on second column.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./x_exp_renal.csv' does not exist: b'./x_exp_renal.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "rn.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Load Data\n",
    "X_renal_data = pd.read_csv('./x_exp_renal.csv', sep='\\t') # Dataset has Donor ID as first column\n",
    "y_renal_data = pd.read_csv('./y_renal.csv', sep=',') # Dataset has Donor ID on first column and Label on second column.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_renal_data.iloc[:,1:],y_renal_data[\"label\"],test_size=0.2, random_state=1) # Drop the Donor ID column from both datasets\n",
    "# Normalization of data sets\n",
    "# Data Scaling MinMax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_norm = X_train\n",
    "X_test_norm = X_test\n",
    "\n",
    "X_train_norm = pd.DataFrame(scaler.fit_transform(X_train_norm))\n",
    "X_test_norm = pd.DataFrame(scaler.transform(X_test_norm))\n",
    "\n",
    "# We will use \"X_train_norm\" as training dataset for the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression: 1025.1\n"
     ]
    }
   ],
   "source": [
    "## AUTOENCODER MODEL\n",
    "# Define the model using the keras functional API\n",
    "def build_autoencoder(encoding_dim: int, number_features: int, regularizer: tf.keras.regularizers.Regularizer, dropout: float):\n",
    "    \"\"\"Two-input autoencoder build function\n",
    "       Parameters: encoding_dim: Size of the latent space (bottleneck layer size).\n",
    "                   number_features: Tuple with the sizes of the two inputs.\n",
    "                   regularizer: keras regularizer object\n",
    "       Returns the 3 models: full autoencoder, the encoder part and the decoder part\n",
    "    \"\"\"\n",
    "    if dropout > 1:\n",
    "        dropout = 1\n",
    "    elif dropout < 0:\n",
    "        dropout = 0\n",
    "    # this is the reduction of our encoded representations, in times.\n",
    "    print(f\"Compression: {number_features/encoding_dim}\")\n",
    "\n",
    "    first_layer_size = number_features/4\n",
    "    second_layer_size = number_features/12\n",
    "    \n",
    "    ## ENCODER\n",
    "    # encoder first input placeholder.\n",
    "    first_input = layers.Input(shape=(number_features))\n",
    "    # encoder first Hidden Layer - H1\n",
    "    H1 = layers.Dense(first_layer_size, activation='relu', kernel_regularizer=regularizer)(first_input)\n",
    "    # encoder first Dropout Layer - D1\n",
    "    D1 = layers.Dropout(dropout)(H1)\n",
    "    # encoder first Batch Normalization Layer - BN1\n",
    "    BN1 = layers.BatchNormalization()(D1)\n",
    "    # encoder second Hidden Layer - H2\n",
    "    H2 = layers.Dense(second_layer_size, activation='relu', kernel_regularizer=regularizer)(BN1)\n",
    "    # encoder second Dropout Layer - D2\n",
    "    D2 = layers.Dropout(dropout)(H2)\n",
    "    # encoder first path second Batch Normalization Layer - BN2\n",
    "    BN2 = layers.BatchNormalization()(D2)\n",
    "\n",
    "   \n",
    "    ## BOTTLENECK \n",
    "    bottleneck = layers.Dense(encoding_dim, activation='relu', kernel_regularizer=regularizer)(BN2)\n",
    "\n",
    "    # this model maps an input to its encoded representation\n",
    "    encoder = keras.models.Model(first_input, bottleneck, name='encoder')\n",
    "\n",
    "    ## DECODER\n",
    "    # Decoder Input Layer - Encoding dimension\n",
    "    encoded_input = layers.Input(shape=(encoding_dim,))\n",
    "    # decoder first Dropout Layer - D3\n",
    "    D3 = layers.Dropout(dropout)(encoded_input)\n",
    "    # decoder first Batch Normalization Layer - BN3 \n",
    "    BN3 = layers.BatchNormalization()(D3)\n",
    "    # decoder first Hidden Layer - H3\n",
    "    H3 = layers.Dense(second_layer_size, activation='relu', kernel_regularizer=regularizer)(BN3)\n",
    "    # decoder second Dropout Layer - D4\n",
    "    D4 = layers.Dropout(dropout)(H3)\n",
    "    # decoder second Batch Normalization Layer - BN4 \n",
    "    BN4 = layers.BatchNormalization()(D4)\n",
    "    # decoder reconstruction layer - O1\n",
    "    O1 = layers.Dense(number_features, activation='sigmoid')(BN4)\n",
    "\n",
    "    # create the decoder model\n",
    "    decoder = keras.models.Model(encoded_input, O1)\n",
    "\n",
    "    # create the full autoencoder\n",
    "    encoder_model = encoder(first_input)\n",
    "    decoder_model = decoder(encoder_model)\n",
    "\n",
    "    autoencoder = keras.models.Model(first_input, decoder_model, name=\"autoencoder\")\n",
    "    \n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# Set Optimizer: Adam with learning rate=0.001\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "# Set Early Stop Callback\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10,  mode='auto', baseline=None, restore_best_weights=False, verbose=1)\n",
    "\n",
    "## Call autoencoder build function and get the AE, the encoder and the decoder.\n",
    "autoencoder, encoder, decoder = build_autoencoder(encoding_dim=20, number_features=X_train_norm.shape[1], regularizer=tf.keras.regularizers.l1_l2(0.0001,0), dropout=0.5)\n",
    "# Compile the autoencoder using Mean Square Error loss function.\n",
    "autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=\"mse\",\n",
    "                        metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING\n",
    "# Fit the training data into the autoencoder.\n",
    "history = autoencoder.fit(X_train_norm,X_train_norm,\n",
    "                          validation_data=(X_test_norm,X_test_norm)\n",
    "                          epochs=200,\n",
    "                          verbose=0,\n",
    "                          callbacks=[early_stop])\n",
    "# Plot training vs validation losses\n",
    "plt.plot(history.history[\"loss\"], c = 'b', label = \"Training\")\n",
    "plt.plot(history.history[\"val_loss\"], c = 'r', label = \"Validation\")\n",
    "plt.title(\"Autoencoder Loss during training epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(history.history[\"loss\"][-1])\n",
    "\n",
    "# Encode datasets using the trained encoder.\n",
    "X_train_encoded = encoder.predict(X_train_norm)\n",
    "X_test_encoded = encoder.predict(X_test_norm)\n",
    "\n",
    "# Renormalize data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_encoded = pd.DataFrame(scaler.fit_transform(X_train_encoded))\n",
    "X_test_encoded = pd.DataFrame(scaler.fit_transform(X_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLASSIFICATION ###\n",
    "# We use the reduced dataset to train a classifier and compare it against the same classifier trained with the original dataset.\n",
    "\n",
    "# One hot encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "OH_encoder = LabelEncoder()\n",
    "OH_y_train = pd.DataFrame(OH_encoder.fit_transform(y_train))\n",
    "OH_y_test = pd.DataFrame(OH_encoder.transform(y_test))\n",
    "y_train_oh = keras.utils.to_categorical(OH_y_train)\n",
    "y_test_oh = keras.utils.to_categorical(OH_y_test)\n",
    "\n",
    "## Definition of the best classifier obtained previously (CTG_dataset_classification)\n",
    "def build_best_model(dropout: int, l1: int, l2: int, input_shape: int):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(100, activation=tf.nn.relu ,kernel_regularizer=keras.regularizers.l1_l2(l1,l2), input_shape=input_shape),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),  \n",
    "        layers.Dense(25,activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l1_l2(l1,l2)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(10,activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l1_l2(l1,l2)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(2,activation=tf.nn.softmax)\n",
    "      ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fit best model with dimensionality reduction data\n",
    "model_ae = build_best_model(0.5,0.0001, 0.00001, X_train_encoded.shape[1])\n",
    "history_ae = model_ae.fit(X_train_pca, y_train_oh, epochs=150,\n",
    "                    validation_split = 0.1, verbose=0, callbacks=[early_stop], shuffle=False)\n",
    "hist_pca = pd.DataFrame(history_ae.history)\n",
    "\n",
    "test_loss, test_acc = model_pca.evaluate(X_test_pca, y_test_oh)\n",
    "\n",
    "# Fit best model with concatenated data\n",
    "model = build_best_model(0.00001, X_train_norm.shape[1])\n",
    "history = model.fit(X_train, y_train_oh, epochs=150,\n",
    "                    validation_split = 0.1, verbose=0, callbacks=[early_stop])\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
