{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-omic Dataset: Latent representation using Denoising Autoencoders\n",
    "- Load Data\n",
    "- Add Swap Noise to Data\n",
    "- Normalize Data\n",
    "- Define Autoencoder Model\n",
    "- Train Autoencoder with normalized noisy dataset\n",
    "- Use transformed dataset for classification\n",
    "- Use transformed dataset for clustering\n",
    "- Evaluation and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swapping: 65 rows.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "rn.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Load Data\n",
    "X_renal_data = pd.read_csv('./x_exp_renal.csv', sep='\\t') # Dataset has Donor ID as first column\n",
    "y_renal_data = pd.read_csv('./y_renal.csv', sep=',') # Dataset has Donor ID on first column and Label on second column.\n",
    "\n",
    "X_train_norm, X_train_swapped, X_test_norm, y_train, y_test, y_train_oh, y_test_oh = Models.prepare_datasets(X_renal_data.iloc[:,1:],y_renal_data[\"label\"], test_size=0.2, swap_noise=0.15)\n",
    "\n",
    "# We will use \"X_train_swapped\" as input training dataset and \"X_train_norm\" as output for the loss function of the Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 433 samples\n",
      "Epoch 1/80\n",
      "433/433 [==============================] - 5s 13ms/sample - loss: 4.8536 - mse: 0.1490\n",
      "Epoch 2/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 1.3967 - mse: 0.1461\n",
      "Epoch 3/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.7701 - mse: 0.1414\n",
      "Epoch 4/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5808 - mse: 0.1329\n",
      "Epoch 5/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5361 - mse: 0.1188\n",
      "Epoch 6/80\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5323 - mse: 0.0988\n",
      "Epoch 7/80\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5896 - mse: 0.0762\n",
      "Epoch 8/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6827 - mse: 0.0553\n",
      "Epoch 9/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.7593 - mse: 0.0406\n",
      "Epoch 10/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.7818 - mse: 0.0321\n",
      "Epoch 11/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.8635 - mse: 0.0255\n",
      "Epoch 12/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 1.0586 - mse: 0.0229\n",
      "Epoch 13/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.9506 - mse: 0.0211\n",
      "Epoch 14/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.8544 - mse: 0.0191\n",
      "Epoch 15/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.8175 - mse: 0.0197\n",
      "Epoch 16/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.7834 - mse: 0.0175\n",
      "Epoch 17/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.7407 - mse: 0.0184\n",
      "Epoch 18/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6765 - mse: 0.0176\n",
      "Epoch 19/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6071 - mse: 0.0173\n",
      "Epoch 20/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5405 - mse: 0.0169\n",
      "Epoch 21/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5075 - mse: 0.0172\n",
      "Epoch 22/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.5064 - mse: 0.0168\n",
      "Epoch 23/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.4895 - mse: 0.0164\n",
      "Epoch 24/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5195 - mse: 0.0164\n",
      "Epoch 25/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5120 - mse: 0.0164\n",
      "Epoch 26/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5566 - mse: 0.0162\n",
      "Epoch 27/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5054 - mse: 0.0161\n",
      "Epoch 28/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5063 - mse: 0.0159\n",
      "Epoch 29/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5105 - mse: 0.0164\n",
      "Epoch 30/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5080 - mse: 0.0155\n",
      "Epoch 31/80\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.5434 - mse: 0.0168\n",
      "Epoch 32/80\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5009 - mse: 0.0152\n",
      "Epoch 33/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.4720 - mse: 0.0163\n",
      "Epoch 34/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.4648 - mse: 0.0159\n",
      "Epoch 35/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.4672 - mse: 0.0153\n",
      "Epoch 36/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.4401 - mse: 0.0154\n",
      "Epoch 37/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.4157 - mse: 0.0157\n",
      "Epoch 38/80\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3747 - mse: 0.0152\n",
      "Epoch 39/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3771 - mse: 0.0153\n",
      "Epoch 40/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.3683 - mse: 0.0152\n",
      "Epoch 41/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3369 - mse: 0.0152\n",
      "Epoch 42/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3751 - mse: 0.0150\n",
      "Epoch 43/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3435 - mse: 0.0148\n",
      "Epoch 44/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3038 - mse: 0.0153\n",
      "Epoch 45/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3928 - mse: 0.0152\n",
      "Epoch 46/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3444 - mse: 0.0154\n",
      "Epoch 47/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6769 - mse: 0.0153\n",
      "Epoch 48/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6359 - mse: 0.0152\n",
      "Epoch 49/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.5324 - mse: 0.0152\n",
      "Epoch 50/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.4468 - mse: 0.0148\n",
      "Epoch 51/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.4253 - mse: 0.0151\n",
      "Epoch 52/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3663 - mse: 0.0147\n",
      "Epoch 53/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3912 - mse: 0.0150\n",
      "Epoch 54/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3817 - mse: 0.0146\n",
      "Epoch 55/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3637 - mse: 0.0148\n",
      "Epoch 56/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3706 - mse: 0.0148\n",
      "Epoch 57/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3462 - mse: 0.0151\n",
      "Epoch 58/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3060 - mse: 0.0145\n",
      "Epoch 59/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2981 - mse: 0.0151\n",
      "Epoch 60/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3574 - mse: 0.0148\n",
      "Epoch 61/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3433 - mse: 0.0152\n",
      "Epoch 62/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3306 - mse: 0.0149\n",
      "Epoch 63/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3511 - mse: 0.0147\n",
      "Epoch 64/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.3039 - mse: 0.0146\n",
      "Epoch 65/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2926 - mse: 0.0145\n",
      "Epoch 66/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2932 - mse: 0.0147\n",
      "Epoch 67/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2753 - mse: 0.0146\n",
      "Epoch 68/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2285 - mse: 0.0143\n",
      "Epoch 69/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2634 - mse: 0.0145\n",
      "Epoch 70/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3020 - mse: 0.0145\n",
      "Epoch 71/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.2814 - mse: 0.0143\n",
      "Epoch 72/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2985 - mse: 0.0145\n",
      "Epoch 73/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3043 - mse: 0.0146\n",
      "Epoch 74/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3503 - mse: 0.0142\n",
      "Epoch 75/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2823 - mse: 0.0147\n",
      "Epoch 76/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2508 - mse: 0.0145\n",
      "Epoch 77/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.2279 - mse: 0.0146\n",
      "Epoch 78/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2366 - mse: 0.0141\n",
      "Epoch 79/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.2322 - mse: 0.0144\n",
      "Epoch 80/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.2526 - mse: 0.0144\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2526181122034306\n"
     ]
    }
   ],
   "source": [
    "###  Hyperparamter tuning\n",
    "# MAP y_train from classes -1 and 1 to 0 and 1\n",
    "y_train_map = ((y_train + 1)/2).astype(int)\n",
    "\n",
    "dropouts = [0.08,0.09,0.01,0.011,0.012]\n",
    "l1 = 0\n",
    "l2 = 0\n",
    "scores = []\n",
    "for dropout in dropouts:\n",
    "    ## Build and Train Autoencoder\n",
    "    autoencoder, encoder, decoder, loss = Models.build_and_train_autoencoder(X_train_norm,\n",
    "                                                                  X_train_norm,\n",
    "                                                                  encoding_dim=50, \n",
    "                                                                  regularizer=tf.keras.regularizers.l1_l2(l1,l2),\n",
    "                                                                  dropout=dropout,\n",
    "                                                                  epochs=200)\n",
    "    ## Encode datasets\n",
    "    X_latent_ae = Models.encode_dataset(X_train_norm, encoder)\n",
    "    X_latent_test_ae = Models.encode_dataset(X_test_norm, encoder)\n",
    "    ### VISUALIZATION WITH PCA\n",
    "    ## ORIGINAL DATASET\n",
    "    X_latent_pca, X_latent_test_pca = Models.perform_PCA(X_train_norm, X_test_norm, y_train, y_test, n_components=10)\n",
    "    ## AUTOENCODER LATENT SPACE\n",
    "    X_latent_pca_ae, X_latent_test_pca_ae = Models.perform_PCA(X_latent_ae, X_latent_test_ae, y_train, y_test, n_components=10)\n",
    "    \n",
    "    ### CLASSIFICATION ###\n",
    "    # We use the reduced dataset to train a classifier and compare it against the same classifier trained with the original dataset.\n",
    "    lr_accuracy, svm_accuracy, rf_accuracy = Models.classify(X_latent_ae, X_latent_test_ae, y_train, y_test, model_type=\"AE\")\n",
    "    \n",
    "    ### CLUSTERING ###\n",
    "    silhouette, mutual_info = Models.cluster(X_latent_ae,y_train_map, model_type=\"AE\")\n",
    "    \n",
    "    scores.append((dropout,l1,l2,loss,lr_accuracy,svm_accuracy,rf_accuracy,silhouette,mutual_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 389 samples, validate on 44 samples\n",
      "Epoch 1/400\n",
      "389/389 [==============================] - 1s 2ms/sample - loss: 1.0320 - accuracy: 0.5707 - val_loss: 0.8316 - val_accuracy: 0.6591\n",
      "Epoch 2/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.8507 - accuracy: 0.6607 - val_loss: 0.7972 - val_accuracy: 0.6591\n",
      "Epoch 3/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.7807 - accuracy: 0.6967 - val_loss: 0.7886 - val_accuracy: 0.6591\n",
      "Epoch 4/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.7179 - accuracy: 0.7378 - val_loss: 0.7845 - val_accuracy: 0.6591\n",
      "Epoch 5/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.7067 - accuracy: 0.7378 - val_loss: 0.7794 - val_accuracy: 0.6591\n",
      "Epoch 6/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6978 - accuracy: 0.7404 - val_loss: 0.7693 - val_accuracy: 0.6591\n",
      "Epoch 7/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.6756 - accuracy: 0.7429 - val_loss: 0.7622 - val_accuracy: 0.6591\n",
      "Epoch 8/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6674 - accuracy: 0.7429 - val_loss: 0.7537 - val_accuracy: 0.6591\n",
      "Epoch 9/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6642 - accuracy: 0.7429 - val_loss: 0.7464 - val_accuracy: 0.6591\n",
      "Epoch 10/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6559 - accuracy: 0.7429 - val_loss: 0.7386 - val_accuracy: 0.6591\n",
      "Epoch 11/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6463 - accuracy: 0.7429 - val_loss: 0.7319 - val_accuracy: 0.6591\n",
      "Epoch 12/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6393 - accuracy: 0.7429 - val_loss: 0.7291 - val_accuracy: 0.6591\n",
      "Epoch 13/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.6333 - accuracy: 0.7429 - val_loss: 0.7216 - val_accuracy: 0.6591\n",
      "Epoch 14/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6319 - accuracy: 0.7429 - val_loss: 0.7182 - val_accuracy: 0.6591\n",
      "Epoch 15/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6241 - accuracy: 0.7429 - val_loss: 0.7150 - val_accuracy: 0.6591\n",
      "Epoch 16/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6203 - accuracy: 0.7429 - val_loss: 0.7116 - val_accuracy: 0.6591\n",
      "Epoch 17/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.71 - 0s 80us/sample - loss: 0.6175 - accuracy: 0.7429 - val_loss: 0.7047 - val_accuracy: 0.6591\n",
      "Epoch 18/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6163 - accuracy: 0.7429 - val_loss: 0.6988 - val_accuracy: 0.6591\n",
      "Epoch 19/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6077 - accuracy: 0.7429 - val_loss: 0.6965 - val_accuracy: 0.6591\n",
      "Epoch 20/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.6076 - accuracy: 0.7429 - val_loss: 0.6969 - val_accuracy: 0.6591\n",
      "Epoch 21/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6060 - accuracy: 0.7429 - val_loss: 0.6953 - val_accuracy: 0.6591\n",
      "Epoch 22/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5992 - accuracy: 0.7429 - val_loss: 0.6908 - val_accuracy: 0.6591\n",
      "Epoch 23/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5960 - accuracy: 0.7429 - val_loss: 0.6864 - val_accuracy: 0.6591\n",
      "Epoch 24/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5935 - accuracy: 0.7429 - val_loss: 0.6867 - val_accuracy: 0.6591\n",
      "Epoch 25/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5935 - accuracy: 0.7429 - val_loss: 0.6861 - val_accuracy: 0.6591\n",
      "Epoch 26/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5902 - accuracy: 0.7429 - val_loss: 0.6889 - val_accuracy: 0.6591\n",
      "Epoch 27/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5915 - accuracy: 0.7429 - val_loss: 0.6833 - val_accuracy: 0.6591\n",
      "Epoch 28/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5863 - accuracy: 0.7429 - val_loss: 0.6801 - val_accuracy: 0.6591\n",
      "Epoch 29/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5841 - accuracy: 0.7429 - val_loss: 0.6812 - val_accuracy: 0.6591\n",
      "Epoch 30/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5866 - accuracy: 0.7429 - val_loss: 0.6849 - val_accuracy: 0.6591\n",
      "Epoch 31/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.6119 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5848 - accuracy: 0.7429 - val_loss: 0.6830 - val_accuracy: 0.6591\n",
      "Epoch 32/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5853 - accuracy: 0.7429 - val_loss: 0.6814 - val_accuracy: 0.6591\n",
      "Epoch 33/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5866 - accuracy: 0.7429 - val_loss: 0.6809 - val_accuracy: 0.6591\n",
      "Epoch 34/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5870 - accuracy: 0.7429 - val_loss: 0.6837 - val_accuracy: 0.6591\n",
      "Epoch 35/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5888 - accuracy: 0.7429 - val_loss: 0.6853 - val_accuracy: 0.6591\n",
      "Epoch 36/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.6161 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5851 - accuracy: 0.7429 - val_loss: 0.6854 - val_accuracy: 0.6591\n",
      "Epoch 37/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5875 - accuracy: 0.7429 - val_loss: 0.6871 - val_accuracy: 0.6591\n",
      "Epoch 38/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5849 - accuracy: 0.7429 - val_loss: 0.6798 - val_accuracy: 0.6591\n",
      "Epoch 39/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5805 - accuracy: 0.7429 - val_loss: 0.6769 - val_accuracy: 0.6591\n",
      "Epoch 40/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5782 - accuracy: 0.7429 - val_loss: 0.6796 - val_accuracy: 0.6591\n",
      "Epoch 41/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5806 - accuracy: 0.7429 - val_loss: 0.6798 - val_accuracy: 0.6591\n",
      "Epoch 42/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5849 - accuracy: 0.7429 - val_loss: 0.6764 - val_accuracy: 0.6591\n",
      "Epoch 43/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5870 - accuracy: 0.7429 - val_loss: 0.6795 - val_accuracy: 0.6591\n",
      "Epoch 44/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5847 - accuracy: 0.7429 - val_loss: 0.6773 - val_accuracy: 0.6591\n",
      "Epoch 45/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5829 - accuracy: 0.7429 - val_loss: 0.6782 - val_accuracy: 0.6591\n",
      "Epoch 46/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5799 - accuracy: 0.7429 - val_loss: 0.6767 - val_accuracy: 0.6591\n",
      "Epoch 47/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5797 - accuracy: 0.7429 - val_loss: 0.6740 - val_accuracy: 0.6591\n",
      "Epoch 48/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5786 - accuracy: 0.7429 - val_loss: 0.6731 - val_accuracy: 0.6591\n",
      "Epoch 49/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5762 - accuracy: 0.7429 - val_loss: 0.6725 - val_accuracy: 0.6591\n",
      "Epoch 50/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5775 - accuracy: 0.7429 - val_loss: 0.6712 - val_accuracy: 0.6591\n",
      "Epoch 51/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5765 - accuracy: 0.7429 - val_loss: 0.6707 - val_accuracy: 0.6591\n",
      "Epoch 52/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5754 - accuracy: 0.7429 - val_loss: 0.6727 - val_accuracy: 0.6591\n",
      "Epoch 53/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5761 - accuracy: 0.7429 - val_loss: 0.6732 - val_accuracy: 0.6591\n",
      "Epoch 54/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5753 - accuracy: 0.7429 - val_loss: 0.6714 - val_accuracy: 0.6591\n",
      "Epoch 55/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5745 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 56/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6669 - val_accuracy: 0.6591\n",
      "Epoch 57/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5744 - accuracy: 0.7429 - val_loss: 0.6672 - val_accuracy: 0.6591\n",
      "Epoch 58/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5742 - accuracy: 0.7429 - val_loss: 0.6686 - val_accuracy: 0.6591\n",
      "Epoch 59/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5746 - accuracy: 0.7429 - val_loss: 0.6713 - val_accuracy: 0.6591\n",
      "Epoch 60/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5741 - accuracy: 0.7429 - val_loss: 0.6694 - val_accuracy: 0.6591\n",
      "Epoch 61/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5733 - accuracy: 0.7429 - val_loss: 0.6665 - val_accuracy: 0.6591\n",
      "Epoch 62/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5737 - accuracy: 0.7429 - val_loss: 0.6667 - val_accuracy: 0.6591\n",
      "Epoch 63/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6650 - val_accuracy: 0.6591\n",
      "Epoch 64/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5748 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 65/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5750 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 66/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5784 - accuracy: 0.7429 - val_loss: 0.6734 - val_accuracy: 0.6591\n",
      "Epoch 67/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5773 - accuracy: 0.7429 - val_loss: 0.6739 - val_accuracy: 0.6591\n",
      "Epoch 68/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5759 - accuracy: 0.7429 - val_loss: 0.6742 - val_accuracy: 0.6591\n",
      "Epoch 69/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5743 - accuracy: 0.7429 - val_loss: 0.6760 - val_accuracy: 0.6591\n",
      "Epoch 70/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5759 - accuracy: 0.7429 - val_loss: 0.6735 - val_accuracy: 0.6591\n",
      "Epoch 71/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5750 - accuracy: 0.7429 - val_loss: 0.6751 - val_accuracy: 0.6591\n",
      "Epoch 72/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5770 - accuracy: 0.7429 - val_loss: 0.6770 - val_accuracy: 0.6591\n",
      "Epoch 73/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5795 - accuracy: 0.7429 - val_loss: 0.6724 - val_accuracy: 0.6591\n",
      "Epoch 74/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5750 - accuracy: 0.7429 - val_loss: 0.6692 - val_accuracy: 0.6591\n",
      "Epoch 75/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5753 - accuracy: 0.7429 - val_loss: 0.6672 - val_accuracy: 0.6591\n",
      "Epoch 76/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5758 - accuracy: 0.7429 - val_loss: 0.6709 - val_accuracy: 0.6591\n",
      "Epoch 77/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5742 - accuracy: 0.7429 - val_loss: 0.6696 - val_accuracy: 0.6591\n",
      "Epoch 78/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5734 - accuracy: 0.7429 - val_loss: 0.6696 - val_accuracy: 0.6591\n",
      "Epoch 79/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5735 - accuracy: 0.7429 - val_loss: 0.6694 - val_accuracy: 0.6591\n",
      "Epoch 80/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5760 - accuracy: 0.7429 - val_loss: 0.6723 - val_accuracy: 0.6591\n",
      "Epoch 81/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5739 - accuracy: 0.7429 - val_loss: 0.6710 - val_accuracy: 0.6591\n",
      "Epoch 82/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5748 - accuracy: 0.7429 - val_loss: 0.6711 - val_accuracy: 0.6591\n",
      "Epoch 83/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5740 - accuracy: 0.7429 - val_loss: 0.6715 - val_accuracy: 0.6591\n",
      "Epoch 84/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5734 - accuracy: 0.7429 - val_loss: 0.6713 - val_accuracy: 0.6591\n",
      "Epoch 85/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.6714 - val_accuracy: 0.6591\n",
      "Epoch 86/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.6721 - val_accuracy: 0.6591\n",
      "Epoch 87/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5742 - accuracy: 0.7429 - val_loss: 0.6721 - val_accuracy: 0.6591\n",
      "Epoch 88/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5743 - accuracy: 0.7429 - val_loss: 0.6708 - val_accuracy: 0.6591\n",
      "Epoch 89/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5743 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 90/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5731 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 91/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5735 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 92/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5730 - accuracy: 0.7429 - val_loss: 0.6674 - val_accuracy: 0.6591\n",
      "Epoch 93/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5724 - accuracy: 0.7429 - val_loss: 0.6675 - val_accuracy: 0.6591\n",
      "Epoch 94/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6674 - val_accuracy: 0.6591\n",
      "Epoch 95/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6676 - val_accuracy: 0.6591\n",
      "Epoch 96/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6676 - val_accuracy: 0.6591\n",
      "Epoch 97/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5714 - accuracy: 0.7429 - val_loss: 0.6671 - val_accuracy: 0.6591\n",
      "Epoch 98/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 99/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 100/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5727 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 101/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 102/400\n",
      "389/389 [==============================] - 0s 201us/sample - loss: 0.5744 - accuracy: 0.7429 - val_loss: 0.6691 - val_accuracy: 0.6591\n",
      "Epoch 103/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.6693 - val_accuracy: 0.6591\n",
      "Epoch 104/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5729 - accuracy: 0.7429 - val_loss: 0.6693 - val_accuracy: 0.6591\n",
      "Epoch 105/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5727 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 106/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6686 - val_accuracy: 0.6591\n",
      "Epoch 107/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.6689 - val_accuracy: 0.6591\n",
      "Epoch 108/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 109/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5731 - accuracy: 0.7429 - val_loss: 0.6690 - val_accuracy: 0.6591\n",
      "Epoch 110/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5729 - accuracy: 0.7429 - val_loss: 0.6692 - val_accuracy: 0.6591\n",
      "Epoch 111/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5722 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 112/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 113/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 114/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 115/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 116/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 117/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 118/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6676 - val_accuracy: 0.6591\n",
      "Epoch 119/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5716 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 120/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 121/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5725 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 122/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6685 - val_accuracy: 0.6591\n",
      "Epoch 123/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6685 - val_accuracy: 0.6591\n",
      "Epoch 124/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 125/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 126/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 127/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 128/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 129/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 130/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 131/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5716 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 132/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 133/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5722 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 134/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 135/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 136/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 137/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 138/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 139/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 140/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 141/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 142/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5716 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 143/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 144/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 145/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 146/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 147/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 148/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5716 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 149/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 150/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 151/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 152/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 153/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 154/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 155/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 156/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 157/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 158/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 159/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 160/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 161/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 162/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 163/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 164/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 165/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 166/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 167/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 168/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 169/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 170/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 171/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 172/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 173/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 174/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 175/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 176/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 177/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 178/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 179/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 180/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 181/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 182/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 183/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 184/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 185/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 186/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 187/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 188/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 189/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 190/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 191/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 192/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 193/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 194/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 195/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 196/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 197/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 198/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 199/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 200/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 201/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 202/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 203/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 204/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 205/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 206/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 207/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 208/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 209/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 210/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 211/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 212/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 213/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 214/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 215/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 216/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 217/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 218/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 219/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 220/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 221/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 222/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 223/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 224/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 225/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 226/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 227/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 228/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 229/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 230/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 231/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 232/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 233/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 234/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 235/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 236/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 237/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 238/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 239/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 240/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 241/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 242/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 243/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 244/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 245/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 246/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 247/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 248/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 249/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 250/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 251/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 252/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 253/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 254/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 255/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 256/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 257/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 258/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 259/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 260/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 261/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 262/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 263/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 264/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 265/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 266/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 267/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.5996 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 268/400\n",
      "389/389 [==============================] - 0s 40us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 269/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.5995 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 270/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 271/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 272/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 273/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 274/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 275/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 276/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 277/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 278/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 279/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 280/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 281/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 282/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 283/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 284/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 285/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 286/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 287/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 288/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 289/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 290/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 291/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 292/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 293/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 294/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 295/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 296/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 297/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 298/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 299/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 300/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 301/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 302/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 303/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 304/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 305/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 306/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 307/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 308/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 309/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 310/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 311/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 312/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 313/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 314/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 315/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 316/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 317/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 318/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 319/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 320/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 321/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 322/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 323/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 324/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 325/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 326/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 327/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 328/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 329/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 330/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 331/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 332/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 333/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 334/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 335/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 336/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 337/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 338/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 339/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 340/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 341/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 342/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 343/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 344/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 345/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 346/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 347/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 348/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 349/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 350/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 351/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 352/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 353/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 354/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 355/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 356/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 357/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 358/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 359/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 360/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 361/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 362/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 363/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 364/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 365/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 366/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 367/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 368/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 369/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 370/400\n",
      "389/389 [==============================] - 0s 40us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 371/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 372/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 373/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 374/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 375/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 376/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 377/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 378/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 379/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 380/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 381/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 382/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 383/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 384/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 385/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 386/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 387/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 388/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 389/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 390/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 391/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 392/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 393/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 394/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 395/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 396/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 397/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 398/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5722 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 399/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 400/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "109/109 [==============================] - 0s 1ms/sample - loss: 0.5928 - accuracy: 0.7248\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFICATION ###\n",
    "# We use the reduced dataset to train a classifier and compare it against the same classifier trained with the original dataset.\n",
    "\n",
    "# One hot encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "OH_encoder = LabelEncoder()\n",
    "OH_y_train = pd.DataFrame(OH_encoder.fit_transform(y_train))\n",
    "OH_y_test = pd.DataFrame(OH_encoder.transform(y_test))\n",
    "y_train_oh = keras.utils.to_categorical(OH_y_train)\n",
    "y_test_oh = keras.utils.to_categorical(OH_y_test)\n",
    "\n",
    "## Definition of the best classifier obtained previously (CTG_dataset_classification)\n",
    "def build_best_model(dropout: int, l1: int, l2: int, input_shape: int):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(30, activation=tf.nn.relu ,kernel_regularizer=keras.regularizers.l1_l2(l1,l2), input_shape=(input_shape,)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(10,activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l1_l2(l1,l2)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(5,activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l1_l2(l1,l2)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(2,activation=tf.nn.softmax)\n",
    "      ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fit best model with dimensionality reduction data\n",
    "model_ae = build_best_model(0.5,0.0001, 0.0001, X_train_encoded.shape[1])\n",
    "history_ae = model_ae.fit(X_train_encoded, y_train_oh, epochs=400,\n",
    "                    validation_split = 0.1, verbose=1, callbacks=[], shuffle=False)\n",
    "hist_ae = pd.DataFrame(history_ae.history)\n",
    "\n",
    "test_loss, test_acc = model_ae.evaluate(X_test_encoded, y_test_oh)\n",
    "\n",
    "# Fit best model with concatenated data\n",
    "#model = build_best_model(0.5,0.0001,0.0001, X_train_norm.shape[1])\n",
    "#history = model.fit(X_train, y_train_oh, epochs=150,\n",
    "#                    validation_split = 0.1, verbose=1, callbacks=[early_stop])\n",
    "#hist = pd.DataFrame(history.history)\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(X_test, y_test_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Here we trained an autoencoder using a dataset modified with noise, and used the original dataset to force the autoencoder to reconstruct it from a smaller latent space. This lead to a latent space of about 1000 times smaller than the original. When compared using classifiers and clustering methods, there is a drop in accuracy and a significant increase in silhouette score. The dimensionality reduction speeds the classifier training process and enables the training of more complex models in reduced amount of time.\n",
    "\n",
    "Classifier Accuracy:\n",
    "\n",
    "Silhouette Score: 0.8439\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
