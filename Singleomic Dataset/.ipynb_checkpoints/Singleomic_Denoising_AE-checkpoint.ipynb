{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-omic Dataset: Latent representation using Denoising Autoencoders\n",
    "- Load Data\n",
    "- Add Swap Noise to Data\n",
    "- Normalize Data\n",
    "- Define Autoencoder Model\n",
    "- Train Autoencoder with normalized noisy dataset\n",
    "- Use transformed dataset for classification\n",
    "- Use transformed dataset for clustering\n",
    "- Evaluation and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swapping: 65 rows.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "rn.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Load Data\n",
    "X_renal_data = pd.read_csv('./x_exp_renal.csv', sep='\\t') # Dataset has Donor ID as first column\n",
    "y_renal_data = pd.read_csv('./y_renal.csv', sep=',') # Dataset has Donor ID on first column and Label on second column.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_renal_data.iloc[:,1:],y_renal_data[\"label\"],test_size=0.2, random_state=1) # Drop the Donor ID column from both datasets\n",
    "\n",
    "X_swapped = X_train\n",
    "\n",
    "# Add swap noise to training dataset\n",
    "# Swap Noise 15% - 1700*0.15 = 255\n",
    "swap_noise = 0.15\n",
    "num_swaps = round(X_train.shape[0]*swap_noise)\n",
    "print(f\"swapping: {num_swaps} rows.\")\n",
    "\n",
    "for col in range(X_train.shape[1]):\n",
    "    to_swap_rows = np.random.randint(X_train.shape[0], size=num_swaps)\n",
    "    sample_rows = np.random.randint(X_train.shape[0], size=num_swaps)\n",
    "    \n",
    "    X_swapped.iloc[to_swap_rows,col] = X_train.iloc[sample_rows,col].values\n",
    "\n",
    "# Normalization of data sets\n",
    "# Data Scaling MinMax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_swapped = X_swapped\n",
    "X_train_norm = X_train\n",
    "X_test_norm = X_test\n",
    "\n",
    "X_train_swapped = pd.DataFrame(scaler.fit_transform(X_train_swapped))\n",
    "X_train_norm = pd.DataFrame(scaler.fit_transform(X_train_norm))\n",
    "X_test_norm = pd.DataFrame(scaler.transform(X_test_norm))\n",
    "\n",
    "# We will use \"X_train_swapped\" as input training dataset and \"X_train_norm\" as output for the loss function of the Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression: 820.08\n"
     ]
    }
   ],
   "source": [
    "## AUTOENCODER MODEL\n",
    "# Define the model using the keras functional API\n",
    "def build_autoencoder(encoding_dim: int, number_features: int, regularizer: tf.keras.regularizers.Regularizer, dropout: float):\n",
    "    \"\"\"Two-input autoencoder build function\n",
    "       Parameters: encoding_dim: Size of the latent space (bottleneck layer size).\n",
    "                   number_features: Tuple with the sizes of the two inputs.\n",
    "                   regularizer: keras regularizer object\n",
    "       Returns the 3 models: full autoencoder, the encoder part and the decoder part\n",
    "    \"\"\"\n",
    "    if dropout > 1:\n",
    "        dropout = 1\n",
    "    elif dropout < 0:\n",
    "        dropout = 0\n",
    "    # this is the reduction of our encoded representations, in times.\n",
    "    print(f\"Compression: {number_features/encoding_dim}\")\n",
    "\n",
    "    first_layer_size = number_features/40\n",
    "    second_layer_size = number_features/150\n",
    "    \n",
    "    ## ENCODER\n",
    "    # encoder first input placeholder.\n",
    "    first_input = layers.Input(shape=(number_features))\n",
    "    # encoder first Hidden Layer - H1\n",
    "    H1 = layers.Dense(first_layer_size, activation='relu', kernel_regularizer=regularizer)(first_input)\n",
    "    # encoder first Dropout Layer - D1\n",
    "    D1 = layers.Dropout(dropout)(H1)\n",
    "    # encoder first Batch Normalization Layer - BN1\n",
    "    BN1 = layers.BatchNormalization()(D1)\n",
    "    # encoder second Hidden Layer - H2\n",
    "    H2 = layers.Dense(second_layer_size, activation='relu', kernel_regularizer=regularizer)(BN1)\n",
    "    # encoder second Dropout Layer - D2\n",
    "    D2 = layers.Dropout(dropout)(H2)\n",
    "    # encoder first path second Batch Normalization Layer - BN2\n",
    "    BN2 = layers.BatchNormalization()(D2)\n",
    "\n",
    "   \n",
    "    ## BOTTLENECK \n",
    "    bottleneck = layers.Dense(encoding_dim, activation='relu', kernel_regularizer=regularizer)(BN2)\n",
    "\n",
    "    # this model maps an input to its encoded representation\n",
    "    encoder = keras.models.Model(first_input, bottleneck, name='encoder')\n",
    "\n",
    "    ## DECODER\n",
    "    # Decoder Input Layer - Encoding dimension\n",
    "    encoded_input = layers.Input(shape=(encoding_dim,))\n",
    "    # decoder first Dropout Layer - D3\n",
    "    D3 = layers.Dropout(dropout)(encoded_input)\n",
    "    # decoder first Batch Normalization Layer - BN3 \n",
    "    BN3 = layers.BatchNormalization()(D3)\n",
    "    # decoder first Hidden Layer - H3\n",
    "    H3 = layers.Dense(second_layer_size, activation='relu', kernel_regularizer=regularizer)(BN3)\n",
    "    # decoder second Dropout Layer - D4\n",
    "    D4 = layers.Dropout(dropout)(H3)\n",
    "    # decoder second Batch Normalization Layer - BN4 \n",
    "    BN4 = layers.BatchNormalization()(D4)\n",
    "    # decoder reconstruction layer - O1\n",
    "    O1 = layers.Dense(number_features, activation='sigmoid')(BN4)\n",
    "\n",
    "    # create the decoder model\n",
    "    decoder = keras.models.Model(encoded_input, O1)\n",
    "\n",
    "    # create the full autoencoder\n",
    "    encoder_model = encoder(first_input)\n",
    "    decoder_model = decoder(encoder_model)\n",
    "\n",
    "    autoencoder = keras.models.Model(first_input, decoder_model, name=\"autoencoder\")\n",
    "    \n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# Set Optimizer: Adam with learning rate=0.001\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "# Set Early Stop Callback\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=10,  mode='auto', baseline=None, restore_best_weights=False, verbose=1)\n",
    "\n",
    "## Call autoencoder build function and get the AE, the encoder and the decoder.\n",
    "autoencoder, encoder, decoder = build_autoencoder(encoding_dim=25, number_features=X_train_swapped.shape[1], regularizer=tf.keras.regularizers.l1_l2(0.0001,0), dropout=0.5)\n",
    "# Compile the autoencoder using Mean Square Error loss function.\n",
    "autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=\"mse\",\n",
    "                        metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 433 samples\n",
      "Epoch 1/80\n",
      "433/433 [==============================] - 5s 13ms/sample - loss: 4.8536 - mse: 0.1490\n",
      "Epoch 2/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 1.3967 - mse: 0.1461\n",
      "Epoch 3/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.7701 - mse: 0.1414\n",
      "Epoch 4/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5808 - mse: 0.1329\n",
      "Epoch 5/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5361 - mse: 0.1188\n",
      "Epoch 6/80\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5323 - mse: 0.0988\n",
      "Epoch 7/80\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5896 - mse: 0.0762\n",
      "Epoch 8/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6827 - mse: 0.0553\n",
      "Epoch 9/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.7593 - mse: 0.0406\n",
      "Epoch 10/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.7818 - mse: 0.0321\n",
      "Epoch 11/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.8635 - mse: 0.0255\n",
      "Epoch 12/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 1.0586 - mse: 0.0229\n",
      "Epoch 13/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.9506 - mse: 0.0211\n",
      "Epoch 14/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.8544 - mse: 0.0191\n",
      "Epoch 15/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.8175 - mse: 0.0197\n",
      "Epoch 16/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.7834 - mse: 0.0175\n",
      "Epoch 17/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.7407 - mse: 0.0184\n",
      "Epoch 18/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6765 - mse: 0.0176\n",
      "Epoch 19/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6071 - mse: 0.0173\n",
      "Epoch 20/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5405 - mse: 0.0169\n",
      "Epoch 21/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5075 - mse: 0.0172\n",
      "Epoch 22/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.5064 - mse: 0.0168\n",
      "Epoch 23/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.4895 - mse: 0.0164\n",
      "Epoch 24/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5195 - mse: 0.0164\n",
      "Epoch 25/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5120 - mse: 0.0164\n",
      "Epoch 26/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5566 - mse: 0.0162\n",
      "Epoch 27/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.5054 - mse: 0.0161\n",
      "Epoch 28/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5063 - mse: 0.0159\n",
      "Epoch 29/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5105 - mse: 0.0164\n",
      "Epoch 30/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5080 - mse: 0.0155\n",
      "Epoch 31/80\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.5434 - mse: 0.0168\n",
      "Epoch 32/80\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5009 - mse: 0.0152\n",
      "Epoch 33/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.4720 - mse: 0.0163\n",
      "Epoch 34/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.4648 - mse: 0.0159\n",
      "Epoch 35/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.4672 - mse: 0.0153\n",
      "Epoch 36/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.4401 - mse: 0.0154\n",
      "Epoch 37/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.4157 - mse: 0.0157\n",
      "Epoch 38/80\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3747 - mse: 0.0152\n",
      "Epoch 39/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3771 - mse: 0.0153\n",
      "Epoch 40/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.3683 - mse: 0.0152\n",
      "Epoch 41/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3369 - mse: 0.0152\n",
      "Epoch 42/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3751 - mse: 0.0150\n",
      "Epoch 43/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3435 - mse: 0.0148\n",
      "Epoch 44/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3038 - mse: 0.0153\n",
      "Epoch 45/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3928 - mse: 0.0152\n",
      "Epoch 46/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3444 - mse: 0.0154\n",
      "Epoch 47/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6769 - mse: 0.0153\n",
      "Epoch 48/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.6359 - mse: 0.0152\n",
      "Epoch 49/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.5324 - mse: 0.0152\n",
      "Epoch 50/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.4468 - mse: 0.0148\n",
      "Epoch 51/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.4253 - mse: 0.0151\n",
      "Epoch 52/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3663 - mse: 0.0147\n",
      "Epoch 53/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3912 - mse: 0.0150\n",
      "Epoch 54/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3817 - mse: 0.0146\n",
      "Epoch 55/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3637 - mse: 0.0148\n",
      "Epoch 56/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3706 - mse: 0.0148\n",
      "Epoch 57/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3462 - mse: 0.0151\n",
      "Epoch 58/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3060 - mse: 0.0145\n",
      "Epoch 59/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2981 - mse: 0.0151\n",
      "Epoch 60/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3574 - mse: 0.0148\n",
      "Epoch 61/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3433 - mse: 0.0152\n",
      "Epoch 62/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3306 - mse: 0.0149\n",
      "Epoch 63/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3511 - mse: 0.0147\n",
      "Epoch 64/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.3039 - mse: 0.0146\n",
      "Epoch 65/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2926 - mse: 0.0145\n",
      "Epoch 66/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2932 - mse: 0.0147\n",
      "Epoch 67/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2753 - mse: 0.0146\n",
      "Epoch 68/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2285 - mse: 0.0143\n",
      "Epoch 69/80\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2634 - mse: 0.0145\n",
      "Epoch 70/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3020 - mse: 0.0145\n",
      "Epoch 71/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.2814 - mse: 0.0143\n",
      "Epoch 72/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2985 - mse: 0.0145\n",
      "Epoch 73/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3043 - mse: 0.0146\n",
      "Epoch 74/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.3503 - mse: 0.0142\n",
      "Epoch 75/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2823 - mse: 0.0147\n",
      "Epoch 76/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2508 - mse: 0.0145\n",
      "Epoch 77/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.2279 - mse: 0.0146\n",
      "Epoch 78/80\n",
      "433/433 [==============================] - 3s 8ms/sample - loss: 0.2366 - mse: 0.0141\n",
      "Epoch 79/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.2322 - mse: 0.0144\n",
      "Epoch 80/80\n",
      "433/433 [==============================] - 4s 8ms/sample - loss: 0.2526 - mse: 0.0144\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2526181122034306\n"
     ]
    }
   ],
   "source": [
    "## TRAINING\n",
    "# Fit the training data into the autoencoder.\n",
    "history = autoencoder.fit(X_train_swapped,X_train_norm,\n",
    "                          epochs=80,\n",
    "                          verbose=1,\n",
    "                          callbacks=[])\n",
    "# Plot training vs validation losses\n",
    "plt.plot(history.history[\"loss\"], c = 'b', label = \"Training\")\n",
    "## plt.plot(history.history[\"val_loss\"], c = 'r', label = \"Validation\")\n",
    "plt.title(\"Autoencoder Loss during training epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(history.history[\"loss\"][-1])\n",
    "\n",
    "# Encode datasets using the trained encoder.\n",
    "X_train_encoded = encoder.predict(X_train_norm)\n",
    "X_test_encoded = encoder.predict(X_test_norm)\n",
    "\n",
    "# Renormalize data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_encoded = pd.DataFrame(scaler.fit_transform(X_train_encoded))\n",
    "X_test_encoded = pd.DataFrame(scaler.fit_transform(X_test_encoded))\n",
    "\n",
    "# Next we will use this datasets to evaluate and compare with the latent representation obtained with the autoencoder:\n",
    "# Original dataset: \"X_train_norm\" a 433x20502 Matrix.\n",
    "# Encoded dataset: \"X_train_encoded\" a 433x{encoded_dim} Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 389 samples, validate on 44 samples\n",
      "Epoch 1/400\n",
      "389/389 [==============================] - 1s 2ms/sample - loss: 1.0320 - accuracy: 0.5707 - val_loss: 0.8316 - val_accuracy: 0.6591\n",
      "Epoch 2/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.8507 - accuracy: 0.6607 - val_loss: 0.7972 - val_accuracy: 0.6591\n",
      "Epoch 3/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.7807 - accuracy: 0.6967 - val_loss: 0.7886 - val_accuracy: 0.6591\n",
      "Epoch 4/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.7179 - accuracy: 0.7378 - val_loss: 0.7845 - val_accuracy: 0.6591\n",
      "Epoch 5/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.7067 - accuracy: 0.7378 - val_loss: 0.7794 - val_accuracy: 0.6591\n",
      "Epoch 6/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6978 - accuracy: 0.7404 - val_loss: 0.7693 - val_accuracy: 0.6591\n",
      "Epoch 7/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.6756 - accuracy: 0.7429 - val_loss: 0.7622 - val_accuracy: 0.6591\n",
      "Epoch 8/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6674 - accuracy: 0.7429 - val_loss: 0.7537 - val_accuracy: 0.6591\n",
      "Epoch 9/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6642 - accuracy: 0.7429 - val_loss: 0.7464 - val_accuracy: 0.6591\n",
      "Epoch 10/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6559 - accuracy: 0.7429 - val_loss: 0.7386 - val_accuracy: 0.6591\n",
      "Epoch 11/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6463 - accuracy: 0.7429 - val_loss: 0.7319 - val_accuracy: 0.6591\n",
      "Epoch 12/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6393 - accuracy: 0.7429 - val_loss: 0.7291 - val_accuracy: 0.6591\n",
      "Epoch 13/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.6333 - accuracy: 0.7429 - val_loss: 0.7216 - val_accuracy: 0.6591\n",
      "Epoch 14/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6319 - accuracy: 0.7429 - val_loss: 0.7182 - val_accuracy: 0.6591\n",
      "Epoch 15/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6241 - accuracy: 0.7429 - val_loss: 0.7150 - val_accuracy: 0.6591\n",
      "Epoch 16/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6203 - accuracy: 0.7429 - val_loss: 0.7116 - val_accuracy: 0.6591\n",
      "Epoch 17/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.71 - 0s 80us/sample - loss: 0.6175 - accuracy: 0.7429 - val_loss: 0.7047 - val_accuracy: 0.6591\n",
      "Epoch 18/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6163 - accuracy: 0.7429 - val_loss: 0.6988 - val_accuracy: 0.6591\n",
      "Epoch 19/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6077 - accuracy: 0.7429 - val_loss: 0.6965 - val_accuracy: 0.6591\n",
      "Epoch 20/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.6076 - accuracy: 0.7429 - val_loss: 0.6969 - val_accuracy: 0.6591\n",
      "Epoch 21/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.6060 - accuracy: 0.7429 - val_loss: 0.6953 - val_accuracy: 0.6591\n",
      "Epoch 22/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5992 - accuracy: 0.7429 - val_loss: 0.6908 - val_accuracy: 0.6591\n",
      "Epoch 23/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5960 - accuracy: 0.7429 - val_loss: 0.6864 - val_accuracy: 0.6591\n",
      "Epoch 24/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5935 - accuracy: 0.7429 - val_loss: 0.6867 - val_accuracy: 0.6591\n",
      "Epoch 25/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5935 - accuracy: 0.7429 - val_loss: 0.6861 - val_accuracy: 0.6591\n",
      "Epoch 26/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5902 - accuracy: 0.7429 - val_loss: 0.6889 - val_accuracy: 0.6591\n",
      "Epoch 27/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5915 - accuracy: 0.7429 - val_loss: 0.6833 - val_accuracy: 0.6591\n",
      "Epoch 28/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5863 - accuracy: 0.7429 - val_loss: 0.6801 - val_accuracy: 0.6591\n",
      "Epoch 29/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5841 - accuracy: 0.7429 - val_loss: 0.6812 - val_accuracy: 0.6591\n",
      "Epoch 30/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5866 - accuracy: 0.7429 - val_loss: 0.6849 - val_accuracy: 0.6591\n",
      "Epoch 31/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.6119 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5848 - accuracy: 0.7429 - val_loss: 0.6830 - val_accuracy: 0.6591\n",
      "Epoch 32/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5853 - accuracy: 0.7429 - val_loss: 0.6814 - val_accuracy: 0.6591\n",
      "Epoch 33/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5866 - accuracy: 0.7429 - val_loss: 0.6809 - val_accuracy: 0.6591\n",
      "Epoch 34/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5870 - accuracy: 0.7429 - val_loss: 0.6837 - val_accuracy: 0.6591\n",
      "Epoch 35/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5888 - accuracy: 0.7429 - val_loss: 0.6853 - val_accuracy: 0.6591\n",
      "Epoch 36/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.6161 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5851 - accuracy: 0.7429 - val_loss: 0.6854 - val_accuracy: 0.6591\n",
      "Epoch 37/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5875 - accuracy: 0.7429 - val_loss: 0.6871 - val_accuracy: 0.6591\n",
      "Epoch 38/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5849 - accuracy: 0.7429 - val_loss: 0.6798 - val_accuracy: 0.6591\n",
      "Epoch 39/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5805 - accuracy: 0.7429 - val_loss: 0.6769 - val_accuracy: 0.6591\n",
      "Epoch 40/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5782 - accuracy: 0.7429 - val_loss: 0.6796 - val_accuracy: 0.6591\n",
      "Epoch 41/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5806 - accuracy: 0.7429 - val_loss: 0.6798 - val_accuracy: 0.6591\n",
      "Epoch 42/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5849 - accuracy: 0.7429 - val_loss: 0.6764 - val_accuracy: 0.6591\n",
      "Epoch 43/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5870 - accuracy: 0.7429 - val_loss: 0.6795 - val_accuracy: 0.6591\n",
      "Epoch 44/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5847 - accuracy: 0.7429 - val_loss: 0.6773 - val_accuracy: 0.6591\n",
      "Epoch 45/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5829 - accuracy: 0.7429 - val_loss: 0.6782 - val_accuracy: 0.6591\n",
      "Epoch 46/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5799 - accuracy: 0.7429 - val_loss: 0.6767 - val_accuracy: 0.6591\n",
      "Epoch 47/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5797 - accuracy: 0.7429 - val_loss: 0.6740 - val_accuracy: 0.6591\n",
      "Epoch 48/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5786 - accuracy: 0.7429 - val_loss: 0.6731 - val_accuracy: 0.6591\n",
      "Epoch 49/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5762 - accuracy: 0.7429 - val_loss: 0.6725 - val_accuracy: 0.6591\n",
      "Epoch 50/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5775 - accuracy: 0.7429 - val_loss: 0.6712 - val_accuracy: 0.6591\n",
      "Epoch 51/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5765 - accuracy: 0.7429 - val_loss: 0.6707 - val_accuracy: 0.6591\n",
      "Epoch 52/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5754 - accuracy: 0.7429 - val_loss: 0.6727 - val_accuracy: 0.6591\n",
      "Epoch 53/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5761 - accuracy: 0.7429 - val_loss: 0.6732 - val_accuracy: 0.6591\n",
      "Epoch 54/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5753 - accuracy: 0.7429 - val_loss: 0.6714 - val_accuracy: 0.6591\n",
      "Epoch 55/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5745 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 56/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6669 - val_accuracy: 0.6591\n",
      "Epoch 57/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5744 - accuracy: 0.7429 - val_loss: 0.6672 - val_accuracy: 0.6591\n",
      "Epoch 58/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5742 - accuracy: 0.7429 - val_loss: 0.6686 - val_accuracy: 0.6591\n",
      "Epoch 59/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5746 - accuracy: 0.7429 - val_loss: 0.6713 - val_accuracy: 0.6591\n",
      "Epoch 60/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5741 - accuracy: 0.7429 - val_loss: 0.6694 - val_accuracy: 0.6591\n",
      "Epoch 61/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5733 - accuracy: 0.7429 - val_loss: 0.6665 - val_accuracy: 0.6591\n",
      "Epoch 62/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5737 - accuracy: 0.7429 - val_loss: 0.6667 - val_accuracy: 0.6591\n",
      "Epoch 63/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6650 - val_accuracy: 0.6591\n",
      "Epoch 64/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5748 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 65/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5750 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 66/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5784 - accuracy: 0.7429 - val_loss: 0.6734 - val_accuracy: 0.6591\n",
      "Epoch 67/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5773 - accuracy: 0.7429 - val_loss: 0.6739 - val_accuracy: 0.6591\n",
      "Epoch 68/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5759 - accuracy: 0.7429 - val_loss: 0.6742 - val_accuracy: 0.6591\n",
      "Epoch 69/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5743 - accuracy: 0.7429 - val_loss: 0.6760 - val_accuracy: 0.6591\n",
      "Epoch 70/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5759 - accuracy: 0.7429 - val_loss: 0.6735 - val_accuracy: 0.6591\n",
      "Epoch 71/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5750 - accuracy: 0.7429 - val_loss: 0.6751 - val_accuracy: 0.6591\n",
      "Epoch 72/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5770 - accuracy: 0.7429 - val_loss: 0.6770 - val_accuracy: 0.6591\n",
      "Epoch 73/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5795 - accuracy: 0.7429 - val_loss: 0.6724 - val_accuracy: 0.6591\n",
      "Epoch 74/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5750 - accuracy: 0.7429 - val_loss: 0.6692 - val_accuracy: 0.6591\n",
      "Epoch 75/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5753 - accuracy: 0.7429 - val_loss: 0.6672 - val_accuracy: 0.6591\n",
      "Epoch 76/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5758 - accuracy: 0.7429 - val_loss: 0.6709 - val_accuracy: 0.6591\n",
      "Epoch 77/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5742 - accuracy: 0.7429 - val_loss: 0.6696 - val_accuracy: 0.6591\n",
      "Epoch 78/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5734 - accuracy: 0.7429 - val_loss: 0.6696 - val_accuracy: 0.6591\n",
      "Epoch 79/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5735 - accuracy: 0.7429 - val_loss: 0.6694 - val_accuracy: 0.6591\n",
      "Epoch 80/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5760 - accuracy: 0.7429 - val_loss: 0.6723 - val_accuracy: 0.6591\n",
      "Epoch 81/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5739 - accuracy: 0.7429 - val_loss: 0.6710 - val_accuracy: 0.6591\n",
      "Epoch 82/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5748 - accuracy: 0.7429 - val_loss: 0.6711 - val_accuracy: 0.6591\n",
      "Epoch 83/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5740 - accuracy: 0.7429 - val_loss: 0.6715 - val_accuracy: 0.6591\n",
      "Epoch 84/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5734 - accuracy: 0.7429 - val_loss: 0.6713 - val_accuracy: 0.6591\n",
      "Epoch 85/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.6714 - val_accuracy: 0.6591\n",
      "Epoch 86/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.6721 - val_accuracy: 0.6591\n",
      "Epoch 87/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5742 - accuracy: 0.7429 - val_loss: 0.6721 - val_accuracy: 0.6591\n",
      "Epoch 88/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5743 - accuracy: 0.7429 - val_loss: 0.6708 - val_accuracy: 0.6591\n",
      "Epoch 89/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5743 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 90/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5731 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 91/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5735 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 92/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5730 - accuracy: 0.7429 - val_loss: 0.6674 - val_accuracy: 0.6591\n",
      "Epoch 93/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5724 - accuracy: 0.7429 - val_loss: 0.6675 - val_accuracy: 0.6591\n",
      "Epoch 94/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6674 - val_accuracy: 0.6591\n",
      "Epoch 95/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6676 - val_accuracy: 0.6591\n",
      "Epoch 96/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6676 - val_accuracy: 0.6591\n",
      "Epoch 97/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5714 - accuracy: 0.7429 - val_loss: 0.6671 - val_accuracy: 0.6591\n",
      "Epoch 98/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 99/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 100/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5727 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 101/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 102/400\n",
      "389/389 [==============================] - 0s 201us/sample - loss: 0.5744 - accuracy: 0.7429 - val_loss: 0.6691 - val_accuracy: 0.6591\n",
      "Epoch 103/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.6693 - val_accuracy: 0.6591\n",
      "Epoch 104/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5729 - accuracy: 0.7429 - val_loss: 0.6693 - val_accuracy: 0.6591\n",
      "Epoch 105/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5727 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 106/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6686 - val_accuracy: 0.6591\n",
      "Epoch 107/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.6689 - val_accuracy: 0.6591\n",
      "Epoch 108/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 109/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5731 - accuracy: 0.7429 - val_loss: 0.6690 - val_accuracy: 0.6591\n",
      "Epoch 110/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5729 - accuracy: 0.7429 - val_loss: 0.6692 - val_accuracy: 0.6591\n",
      "Epoch 111/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5722 - accuracy: 0.7429 - val_loss: 0.6687 - val_accuracy: 0.6591\n",
      "Epoch 112/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 113/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 114/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 115/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 116/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 117/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 118/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6676 - val_accuracy: 0.6591\n",
      "Epoch 119/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5716 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 120/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 121/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5725 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 122/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5723 - accuracy: 0.7429 - val_loss: 0.6685 - val_accuracy: 0.6591\n",
      "Epoch 123/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6685 - val_accuracy: 0.6591\n",
      "Epoch 124/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 125/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 126/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 127/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 128/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 129/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 130/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 131/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5716 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 132/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 133/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5722 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 134/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 135/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 136/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 137/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 138/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 139/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 140/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 141/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 142/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5716 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 143/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 144/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 145/400\n",
      "389/389 [==============================] - 0s 161us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 146/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 147/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 148/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5716 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 149/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 150/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6678 - val_accuracy: 0.6591\n",
      "Epoch 151/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 152/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6677 - val_accuracy: 0.6591\n",
      "Epoch 153/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5717 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 154/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 155/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 156/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 157/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 158/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 159/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 160/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 161/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 162/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 163/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 164/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 165/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 166/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 167/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 168/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 169/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 170/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 171/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 172/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 173/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 174/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 175/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6679 - val_accuracy: 0.6591\n",
      "Epoch 176/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 177/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 178/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 179/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 180/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 181/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 182/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 183/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 184/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 185/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 186/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 187/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 188/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 189/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 190/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 191/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 192/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 193/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 194/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 195/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 196/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 197/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 198/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 199/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 200/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 201/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 202/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 203/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 204/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5718 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 205/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 206/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 207/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 208/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 209/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 210/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 211/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 212/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 213/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 214/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 215/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 216/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 217/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 218/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6680 - val_accuracy: 0.6591\n",
      "Epoch 219/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 220/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 221/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 222/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 223/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 224/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 225/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 226/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 227/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 228/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 229/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 230/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 231/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 232/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 233/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 234/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 235/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 236/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 237/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 238/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 239/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 240/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 241/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 242/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 243/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 244/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 245/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 246/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 247/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 248/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 249/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 250/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 251/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 252/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 253/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 254/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 255/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 256/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 257/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 258/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 259/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5719 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 260/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 261/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 262/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 263/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 264/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 265/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6681 - val_accuracy: 0.6591\n",
      "Epoch 266/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 267/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.5996 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 268/400\n",
      "389/389 [==============================] - 0s 40us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 269/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.5995 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 270/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 271/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 272/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 273/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 274/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 275/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 276/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 277/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 278/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 279/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 280/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 281/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 282/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 283/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 284/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 285/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 286/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 287/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 288/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 289/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 290/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 291/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 292/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 293/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 294/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 295/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 296/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 297/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 298/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 299/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 300/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 301/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 302/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 303/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 304/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 305/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 306/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 307/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 308/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 309/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 310/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 311/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 312/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 313/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 314/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 315/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 316/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 317/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 318/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 319/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 320/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5720 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 321/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 322/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 323/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 324/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 325/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6682 - val_accuracy: 0.6591\n",
      "Epoch 326/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 327/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 328/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 329/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 330/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 331/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 332/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 333/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 334/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 335/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 336/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 337/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 338/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 339/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 340/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 341/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 342/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 343/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 344/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 345/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 346/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 347/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 348/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 349/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 350/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 351/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 352/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 353/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 354/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 355/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 356/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 357/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 358/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 359/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 360/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 361/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 362/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 363/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 364/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 365/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 366/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 367/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 368/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 369/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 370/400\n",
      "389/389 [==============================] - 0s 40us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 371/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 372/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 373/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 374/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 375/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 376/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 377/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 378/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 379/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 380/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 381/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 382/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 383/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 384/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 385/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 386/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 387/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 388/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 389/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 390/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 391/400\n",
      "389/389 [==============================] - 0s 120us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 392/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 393/400\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.71 - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 394/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 395/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 396/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6684 - val_accuracy: 0.6591\n",
      "Epoch 397/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 398/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5722 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 399/400\n",
      "389/389 [==============================] - 0s 121us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "Epoch 400/400\n",
      "389/389 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7429 - val_loss: 0.6683 - val_accuracy: 0.6591\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "109/109 [==============================] - 0s 1ms/sample - loss: 0.5928 - accuracy: 0.7248\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFICATION ###\n",
    "# We use the reduced dataset to train a classifier and compare it against the same classifier trained with the original dataset.\n",
    "\n",
    "# One hot encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "OH_encoder = LabelEncoder()\n",
    "OH_y_train = pd.DataFrame(OH_encoder.fit_transform(y_train))\n",
    "OH_y_test = pd.DataFrame(OH_encoder.transform(y_test))\n",
    "y_train_oh = keras.utils.to_categorical(OH_y_train)\n",
    "y_test_oh = keras.utils.to_categorical(OH_y_test)\n",
    "\n",
    "## Definition of the best classifier obtained previously (CTG_dataset_classification)\n",
    "def build_best_model(dropout: int, l1: int, l2: int, input_shape: int):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(30, activation=tf.nn.relu ,kernel_regularizer=keras.regularizers.l1_l2(l1,l2), input_shape=(input_shape,)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(10,activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l1_l2(l1,l2)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(5,activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l1_l2(l1,l2)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(2,activation=tf.nn.softmax)\n",
    "      ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fit best model with dimensionality reduction data\n",
    "model_ae = build_best_model(0.5,0.0001, 0.0001, X_train_encoded.shape[1])\n",
    "history_ae = model_ae.fit(X_train_encoded, y_train_oh, epochs=400,\n",
    "                    validation_split = 0.1, verbose=1, callbacks=[], shuffle=False)\n",
    "hist_ae = pd.DataFrame(history_ae.history)\n",
    "\n",
    "test_loss, test_acc = model_ae.evaluate(X_test_encoded, y_test_oh)\n",
    "\n",
    "# Fit best model with concatenated data\n",
    "#model = build_best_model(0.5,0.0001,0.0001, X_train_norm.shape[1])\n",
    "#history = model.fit(X_train, y_train_oh, epochs=150,\n",
    "#                    validation_split = 0.1, verbose=1, callbacks=[early_stop])\n",
    "#hist = pd.DataFrame(history.history)\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(X_test, y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE silhoutte score: 0.8439498543739319\n",
      "Original silhoutte score: 0.3350685056729452\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### CLUSTERING ###\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "dae_cluster_labels = kmeans.fit_predict(X_train_encoded)\n",
    "dae_silhouette_avg = silhouette_score(X_train_encoded, dae_cluster_labels)\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(X_train)\n",
    "silhouette_avg = silhouette_score(X_train, cluster_labels)\n",
    "\n",
    "print(f\"AE silhoutte score: {dae_silhouette_avg}\")\n",
    "print(f\"Original silhoutte score: {silhouette_avg}\")\n",
    "\n",
    "### PLOT SILOHUETTE SCORE FOR CLUSTERS\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(18, 7)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1 but in this example all\n",
    "# lie within [-0.1, 1]\n",
    "ax1.set_xlim([-0.1, 1])\n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, len(X_train_encoded) + (n_clusters + 1) * 10])\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X_train_encoded, dae_cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                      0, ith_cluster_silhouette_values,\n",
    "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax1.axvline(x=dae_silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X_train, cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax2.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                      0, ith_cluster_silhouette_values,\n",
    "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax2.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax2.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax2.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax2.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax2.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax2.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Here we trained an autoencoder using a dataset modified with noise, and used the original dataset to force the autoencoder to reconstruct it from a smaller latent space. This lead to a latent space of about 1000 times smaller than the original. When compared using classifiers and clustering methods, there is a drop in accuracy and a significant increase in silhouette score. The dimensionality reduction speeds the classifier training process and enables the training of more complex models in reduced amount of time.\n",
    "\n",
    "Classifier Accuracy:\n",
    "\n",
    "Silhouette Score: 0.8439\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
