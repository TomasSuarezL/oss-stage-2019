{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-omic Dataset: Latent representation using Autoencoders\n",
    "- Load Data\n",
    "- Normalize Data\n",
    "- Define Autoencoder Model\n",
    "- Train Autoencoder with normalized dataset\n",
    "- Use transformed dataset for classification\n",
    "- Use transformed dataset for clustering\n",
    "- Evaluation and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "rn.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Load Data\n",
    "X_renal_data = pd.read_csv('./x_exp_renal.csv', sep='\\t') # Dataset has Donor ID as first column\n",
    "y_renal_data = pd.read_csv('./y_renal.csv', sep=',') # Dataset has Donor ID on first column and Label on second column.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_renal_data.iloc[:,1:],y_renal_data[\"label\"],test_size=0.2, random_state=1) # Drop the Donor ID column from both datasets\n",
    "\n",
    "# Normalization of data sets\n",
    "# Data Scaling MinMax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_norm = X_train\n",
    "X_test_norm = X_test\n",
    "\n",
    "X_train_norm = pd.DataFrame(scaler.fit_transform(X_train_norm))\n",
    "X_test_norm = pd.DataFrame(scaler.transform(X_test_norm))\n",
    "\n",
    "# We will use \"X_train_norm\" as training dataset for the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression: 1025.1\n"
     ]
    }
   ],
   "source": [
    "## AUTOENCODER MODEL\n",
    "# Define the model using the keras functional API\n",
    "def build_autoencoder(encoding_dim: int, number_features: int, regularizer: tf.keras.regularizers.Regularizer, dropout: float):\n",
    "    \"\"\"Two-input autoencoder build function\n",
    "       Parameters: encoding_dim: Size of the latent space (bottleneck layer size).\n",
    "                   number_features: Tuple with the sizes of the two inputs.\n",
    "                   regularizer: keras regularizer object\n",
    "       Returns the 3 models: full autoencoder, the encoder part and the decoder part\n",
    "    \"\"\"\n",
    "    if dropout > 1:\n",
    "        dropout = 1\n",
    "    elif dropout < 0:\n",
    "        dropout = 0\n",
    "    # this is the reduction of our encoded representations, in times.\n",
    "    print(f\"Compression: {number_features/encoding_dim}\")\n",
    "\n",
    "    first_layer_size = number_features/40\n",
    "    second_layer_size = number_features/120\n",
    "    \n",
    "    ## ENCODER\n",
    "    # encoder first input placeholder.\n",
    "    first_input = layers.Input(shape=(number_features))\n",
    "    # encoder first Hidden Layer - H1\n",
    "    H1 = layers.Dense(first_layer_size, activation='relu', kernel_regularizer=regularizer)(first_input)\n",
    "    # encoder first Dropout Layer - D1\n",
    "    D1 = layers.Dropout(dropout)(H1)\n",
    "    # encoder first Batch Normalization Layer - BN1\n",
    "    BN1 = layers.BatchNormalization()(D1)\n",
    "    # encoder second Hidden Layer - H2\n",
    "    H2 = layers.Dense(second_layer_size, activation='relu', kernel_regularizer=regularizer)(BN1)\n",
    "    # encoder second Dropout Layer - D2\n",
    "    D2 = layers.Dropout(dropout)(H2)\n",
    "    # encoder first path second Batch Normalization Layer - BN2\n",
    "    BN2 = layers.BatchNormalization()(D2)\n",
    "\n",
    "   \n",
    "    ## BOTTLENECK \n",
    "    bottleneck = layers.Dense(encoding_dim, activation='relu', kernel_regularizer=regularizer)(BN2)\n",
    "\n",
    "    # this model maps an input to its encoded representation\n",
    "    encoder = keras.models.Model(first_input, bottleneck, name='encoder')\n",
    "\n",
    "    ## DECODER\n",
    "    # Decoder Input Layer - Encoding dimension\n",
    "    encoded_input = layers.Input(shape=(encoding_dim,))\n",
    "    # decoder first Dropout Layer - D3\n",
    "    D3 = layers.Dropout(dropout)(encoded_input)\n",
    "    # decoder first Batch Normalization Layer - BN3 \n",
    "    BN3 = layers.BatchNormalization()(D3)\n",
    "    # decoder first Hidden Layer - H3\n",
    "    H3 = layers.Dense(second_layer_size, activation='relu', kernel_regularizer=regularizer)(BN3)\n",
    "    # decoder second Dropout Layer - D4\n",
    "    D4 = layers.Dropout(dropout)(H3)\n",
    "    # decoder second Batch Normalization Layer - BN4 \n",
    "    BN4 = layers.BatchNormalization()(D4)\n",
    "    # decoder reconstruction layer - O1\n",
    "    O1 = layers.Dense(number_features, activation='sigmoid')(BN4)\n",
    "\n",
    "    # create the decoder model\n",
    "    decoder = keras.models.Model(encoded_input, O1)\n",
    "\n",
    "    # create the full autoencoder\n",
    "    encoder_model = encoder(first_input)\n",
    "    decoder_model = decoder(encoder_model)\n",
    "\n",
    "    autoencoder = keras.models.Model(first_input, decoder_model, name=\"autoencoder\")\n",
    "    \n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# Set Optimizer: Adam with learning rate=0.001\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "# Set Early Stop Callback\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=10,  mode='auto', baseline=None, restore_best_weights=False, verbose=1)\n",
    "\n",
    "## Call autoencoder build function and get the AE, the encoder and the decoder.\n",
    "autoencoder, encoder, decoder = build_autoencoder(encoding_dim=20, number_features=X_train_norm.shape[1], regularizer=tf.keras.regularizers.l1_l2(0.0001,0), dropout=0.5)\n",
    "# Compile the autoencoder using Mean Square Error loss function.\n",
    "autoencoder.compile(optimizer=optimizer,\n",
    "                        loss=\"mse\",\n",
    "                        metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5401 - mse: 0.0156 - val_loss: 0.5858 - val_mse: 0.0581\n",
      "Epoch 2/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5828 - mse: 0.0161 - val_loss: 0.6273 - val_mse: 0.0561\n",
      "Epoch 3/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5345 - mse: 0.0149 - val_loss: 0.4978 - val_mse: 0.0587\n",
      "Epoch 4/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.4817 - mse: 0.0148 - val_loss: 0.5164 - val_mse: 0.0577\n",
      "Epoch 5/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.4764 - mse: 0.0142 - val_loss: 0.5032 - val_mse: 0.0566\n",
      "Epoch 6/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.4707 - mse: 0.0142 - val_loss: 0.5292 - val_mse: 0.0561\n",
      "Epoch 7/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5138 - mse: 0.0140 - val_loss: 0.5388 - val_mse: 0.0572\n",
      "Epoch 8/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.4705 - mse: 0.0143 - val_loss: 0.5056 - val_mse: 0.0572\n",
      "Epoch 9/100\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.5072 - mse: 0.0143 - val_loss: 0.8440 - val_mse: 0.0548\n",
      "Epoch 10/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.7829 - mse: 0.0135 - val_loss: 0.7174 - val_mse: 0.0580\n",
      "Epoch 11/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.6149 - mse: 0.0138 - val_loss: 0.6075 - val_mse: 0.0571\n",
      "Epoch 12/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5236 - mse: 0.0147 - val_loss: 0.5515 - val_mse: 0.0557\n",
      "Epoch 13/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5299 - mse: 0.0134 - val_loss: 0.5430 - val_mse: 0.0573\n",
      "Epoch 14/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.5313 - mse: 0.0143 - val_loss: 0.6169 - val_mse: 0.0550\n",
      "Epoch 15/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.5404 - mse: 0.0142 - val_loss: 0.5272 - val_mse: 0.0564\n",
      "Epoch 16/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.4401 - mse: 0.0133 - val_loss: 0.4294 - val_mse: 0.0574\n",
      "Epoch 17/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.4300 - mse: 0.0141 - val_loss: 0.4811 - val_mse: 0.0558\n",
      "Epoch 18/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3929 - mse: 0.0134 - val_loss: 0.4223 - val_mse: 0.0578\n",
      "Epoch 19/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.4472 - mse: 0.0134 - val_loss: 0.4980 - val_mse: 0.0562\n",
      "Epoch 20/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.4368 - mse: 0.0140 - val_loss: 0.4844 - val_mse: 0.0555\n",
      "Epoch 21/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.4417 - mse: 0.0134 - val_loss: 0.4920 - val_mse: 0.0550\n",
      "Epoch 22/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.4583 - mse: 0.0138 - val_loss: 0.4836 - val_mse: 0.0561\n",
      "Epoch 23/100\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.4044 - mse: 0.0134 - val_loss: 0.4085 - val_mse: 0.0586\n",
      "Epoch 24/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3378 - mse: 0.0128 - val_loss: 0.3801 - val_mse: 0.0564\n",
      "Epoch 25/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3396 - mse: 0.0132 - val_loss: 0.3691 - val_mse: 0.0584\n",
      "Epoch 26/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3335 - mse: 0.0134 - val_loss: 0.4373 - val_mse: 0.0546\n",
      "Epoch 27/100\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.3781 - mse: 0.0131 - val_loss: 0.4023 - val_mse: 0.0567\n",
      "Epoch 28/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3765 - mse: 0.0135 - val_loss: 0.4348 - val_mse: 0.0552\n",
      "Epoch 29/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3695 - mse: 0.0132 - val_loss: 0.3907 - val_mse: 0.0569\n",
      "Epoch 30/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3499 - mse: 0.0135 - val_loss: 0.4248 - val_mse: 0.0548\n",
      "Epoch 31/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3802 - mse: 0.0123 - val_loss: 0.3634 - val_mse: 0.0567\n",
      "Epoch 32/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3095 - mse: 0.0124 - val_loss: 0.3502 - val_mse: 0.0576\n",
      "Epoch 33/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3048 - mse: 0.0126 - val_loss: 0.3567 - val_mse: 0.0566\n",
      "Epoch 34/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2944 - mse: 0.0127 - val_loss: 0.3575 - val_mse: 0.0559\n",
      "Epoch 35/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2981 - mse: 0.0122 - val_loss: 0.3459 - val_mse: 0.0574\n",
      "Epoch 36/100\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.3110 - mse: 0.0126 - val_loss: 0.3729 - val_mse: 0.0552\n",
      "Epoch 37/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3450 - mse: 0.0125 - val_loss: 0.3854 - val_mse: 0.0566\n",
      "Epoch 38/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3817 - mse: 0.0127 - val_loss: 0.3997 - val_mse: 0.0567\n",
      "Epoch 39/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3286 - mse: 0.0129 - val_loss: 0.3338 - val_mse: 0.0555\n",
      "Epoch 40/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2968 - mse: 0.0126 - val_loss: 0.3418 - val_mse: 0.0560\n",
      "Epoch 41/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2903 - mse: 0.0127 - val_loss: 0.3346 - val_mse: 0.0556\n",
      "Epoch 42/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2994 - mse: 0.0131 - val_loss: 0.3184 - val_mse: 0.0557\n",
      "Epoch 43/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2869 - mse: 0.0124 - val_loss: 0.3586 - val_mse: 0.0554\n",
      "Epoch 44/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3136 - mse: 0.0124 - val_loss: 0.3421 - val_mse: 0.0544\n",
      "Epoch 45/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2897 - mse: 0.0125 - val_loss: 0.3209 - val_mse: 0.0547\n",
      "Epoch 46/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2673 - mse: 0.0123 - val_loss: 0.3218 - val_mse: 0.0546\n",
      "Epoch 47/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3358 - mse: 0.0126 - val_loss: 0.3848 - val_mse: 0.0565\n",
      "Epoch 48/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3257 - mse: 0.0126 - val_loss: 0.3650 - val_mse: 0.0549\n",
      "Epoch 49/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3048 - mse: 0.0125 - val_loss: 0.3299 - val_mse: 0.0544\n",
      "Epoch 50/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.3066 - mse: 0.0120 - val_loss: 0.3317 - val_mse: 0.0561\n",
      "Epoch 51/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2891 - mse: 0.0122 - val_loss: 0.3156 - val_mse: 0.0566\n",
      "Epoch 52/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2625 - mse: 0.0123 - val_loss: 0.2856 - val_mse: 0.0563\n",
      "Epoch 53/100\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.2513 - mse: 0.0128 - val_loss: 0.2972 - val_mse: 0.0560\n",
      "Epoch 54/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2488 - mse: 0.0127 - val_loss: 0.2826 - val_mse: 0.0553\n",
      "Epoch 55/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2430 - mse: 0.0124 - val_loss: 0.3074 - val_mse: 0.0548\n",
      "Epoch 56/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2567 - mse: 0.0122 - val_loss: 0.3039 - val_mse: 0.0538\n",
      "Epoch 57/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2520 - mse: 0.0119 - val_loss: 0.2678 - val_mse: 0.0543\n",
      "Epoch 58/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2219 - mse: 0.0123 - val_loss: 0.2606 - val_mse: 0.0562\n",
      "Epoch 59/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2098 - mse: 0.0122 - val_loss: 0.2711 - val_mse: 0.0535\n",
      "Epoch 60/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2334 - mse: 0.0120 - val_loss: 0.2852 - val_mse: 0.0548\n",
      "Epoch 61/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2383 - mse: 0.0118 - val_loss: 0.3112 - val_mse: 0.0547\n",
      "Epoch 62/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2628 - mse: 0.0119 - val_loss: 0.2829 - val_mse: 0.0550\n",
      "Epoch 63/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2669 - mse: 0.0124 - val_loss: 0.3305 - val_mse: 0.0542\n",
      "Epoch 64/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2672 - mse: 0.0121 - val_loss: 0.3470 - val_mse: 0.0539\n",
      "Epoch 65/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.3129 - mse: 0.0117 - val_loss: 0.3424 - val_mse: 0.0536\n",
      "Epoch 66/100\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.2854 - mse: 0.0119 - val_loss: 0.3043 - val_mse: 0.0537\n",
      "Epoch 67/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2481 - mse: 0.0121 - val_loss: 0.2908 - val_mse: 0.0558\n",
      "Epoch 68/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2311 - mse: 0.0118 - val_loss: 0.2523 - val_mse: 0.0543\n",
      "Epoch 69/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2086 - mse: 0.0118 - val_loss: 0.2494 - val_mse: 0.0538\n",
      "Epoch 70/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2024 - mse: 0.0116 - val_loss: 0.2400 - val_mse: 0.0575\n",
      "Epoch 71/100\n",
      "433/433 [==============================] - 4s 9ms/sample - loss: 0.2152 - mse: 0.0118 - val_loss: 0.2863 - val_mse: 0.0543\n",
      "Epoch 72/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2327 - mse: 0.0117 - val_loss: 0.2732 - val_mse: 0.0557\n",
      "Epoch 73/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2316 - mse: 0.0122 - val_loss: 0.2653 - val_mse: 0.0536\n",
      "Epoch 74/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2151 - mse: 0.0117 - val_loss: 0.2340 - val_mse: 0.0550\n",
      "Epoch 75/100\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.1855 - mse: 0.0116 - val_loss: 0.2267 - val_mse: 0.0536\n",
      "Epoch 76/100\n",
      "433/433 [==============================] - 5s 12ms/sample - loss: 0.1841 - mse: 0.0118 - val_loss: 0.2238 - val_mse: 0.0559\n",
      "Epoch 77/100\n",
      "433/433 [==============================] - 5s 11ms/sample - loss: 0.1831 - mse: 0.0116 - val_loss: 0.2296 - val_mse: 0.0541\n",
      "Epoch 78/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.1796 - mse: 0.0118 - val_loss: 0.2260 - val_mse: 0.0555\n",
      "Epoch 79/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.1817 - mse: 0.0113 - val_loss: 0.2346 - val_mse: 0.0539\n",
      "Epoch 80/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2030 - mse: 0.0118 - val_loss: 0.2635 - val_mse: 0.0572\n",
      "Epoch 81/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2254 - mse: 0.0119 - val_loss: 0.2496 - val_mse: 0.0536\n",
      "Epoch 82/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2093 - mse: 0.0121 - val_loss: 0.2413 - val_mse: 0.0535\n",
      "Epoch 83/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.1960 - mse: 0.0118 - val_loss: 0.2433 - val_mse: 0.0543\n",
      "Epoch 84/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2080 - mse: 0.0116 - val_loss: 0.2578 - val_mse: 0.0580\n",
      "Epoch 85/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.1998 - mse: 0.0119 - val_loss: 0.2495 - val_mse: 0.0547\n",
      "Epoch 86/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2140 - mse: 0.0119 - val_loss: 0.2528 - val_mse: 0.0539\n",
      "Epoch 87/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2017 - mse: 0.0120 - val_loss: 0.2614 - val_mse: 0.0570\n",
      "Epoch 88/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2117 - mse: 0.0117 - val_loss: 0.2715 - val_mse: 0.0651\n",
      "Epoch 89/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2310 - mse: 0.0118 - val_loss: 0.2739 - val_mse: 0.0552\n",
      "Epoch 90/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2248 - mse: 0.0113 - val_loss: 0.2391 - val_mse: 0.0534\n",
      "Epoch 91/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.1945 - mse: 0.0117 - val_loss: 0.2369 - val_mse: 0.0561\n",
      "Epoch 92/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2044 - mse: 0.0119 - val_loss: 0.2349 - val_mse: 0.0541\n",
      "Epoch 93/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.1849 - mse: 0.0114 - val_loss: 0.2230 - val_mse: 0.0534\n",
      "Epoch 94/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.1849 - mse: 0.0116 - val_loss: 0.2345 - val_mse: 0.0533\n",
      "Epoch 95/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.1988 - mse: 0.0116 - val_loss: 0.2386 - val_mse: 0.0532\n",
      "Epoch 96/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2125 - mse: 0.0120 - val_loss: 0.2591 - val_mse: 0.0535\n",
      "Epoch 97/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2191 - mse: 0.0120 - val_loss: 0.3491 - val_mse: 0.1042\n",
      "Epoch 98/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2491 - mse: 0.0122 - val_loss: 0.2815 - val_mse: 0.0531\n",
      "Epoch 99/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2254 - mse: 0.0120 - val_loss: 0.2629 - val_mse: 0.0541\n",
      "Epoch 100/100\n",
      "433/433 [==============================] - 4s 10ms/sample - loss: 0.2152 - mse: 0.0115 - val_loss: 0.2349 - val_mse: 0.0545\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2152251326597315\n"
     ]
    }
   ],
   "source": [
    "## TRAINING\n",
    "# Fit the training data into the autoencoder.\n",
    "history = autoencoder.fit(X_train_norm,X_train_norm,\n",
    "                          validation_data=(X_test_norm,X_test_norm),\n",
    "                          epochs=100,\n",
    "                          verbose=1,\n",
    "                          callbacks=[])\n",
    "# Plot training vs validation losses\n",
    "plt.plot(history.history[\"loss\"], c = 'b', label = \"Training\")\n",
    "plt.plot(history.history[\"val_loss\"], c = 'r', label = \"Validation\")\n",
    "plt.title(\"Autoencoder Loss during training epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(history.history[\"loss\"][-1])\n",
    "\n",
    "# Encode datasets using the trained encoder.\n",
    "X_train_encoded = encoder.predict(X_train_norm)\n",
    "X_test_encoded = encoder.predict(X_test_norm)\n",
    "\n",
    "# Renormalize data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_encoded = pd.DataFrame(scaler.fit_transform(X_train_encoded))\n",
    "X_test_encoded = pd.DataFrame(scaler.fit_transform(X_test_encoded))\n",
    "\n",
    "# Next we will use this datasets to evaluate and compare with the latent representation obtained with the autoencoder:\n",
    "# Original dataset: \"X_train_norm\" a 433x20502 Matrix.\n",
    "# Encoded dataset: \"X_train_encoded\" a 433x{encoded_dim} Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 411 samples, validate on 22 samples\n",
      "Epoch 1/300\n",
      "411/411 [==============================] - 1s 3ms/sample - loss: 2.2368 - accuracy: 0.5061 - val_loss: 1.8269 - val_accuracy: 0.5455\n",
      "Epoch 2/300\n",
      "411/411 [==============================] - 0s 548us/sample - loss: 2.1704 - accuracy: 0.4891 - val_loss: 1.7179 - val_accuracy: 0.5455\n",
      "Epoch 3/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 1.9214 - accuracy: 0.5742 - val_loss: 1.6766 - val_accuracy: 0.5455\n",
      "Epoch 4/300\n",
      "411/411 [==============================] - 0s 546us/sample - loss: 1.9336 - accuracy: 0.5353 - val_loss: 1.6373 - val_accuracy: 0.5455\n",
      "Epoch 5/300\n",
      "411/411 [==============================] - 0s 517us/sample - loss: 1.7853 - accuracy: 0.5839 - val_loss: 1.6018 - val_accuracy: 0.6364\n",
      "Epoch 6/300\n",
      "411/411 [==============================] - 0s 587us/sample - loss: 1.7266 - accuracy: 0.6010 - val_loss: 1.5710 - val_accuracy: 0.7273\n",
      "Epoch 7/300\n",
      "411/411 [==============================] - 0s 757us/sample - loss: 1.7517 - accuracy: 0.6180 - val_loss: 1.5485 - val_accuracy: 0.7273\n",
      "Epoch 8/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 1.7143 - accuracy: 0.6156 - val_loss: 1.5283 - val_accuracy: 0.7273\n",
      "Epoch 9/300\n",
      "411/411 [==============================] - 0s 558us/sample - loss: 1.6534 - accuracy: 0.6448 - val_loss: 1.5181 - val_accuracy: 0.7273\n",
      "Epoch 10/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 1.7293 - accuracy: 0.6204 - val_loss: 1.5030 - val_accuracy: 0.7273\n",
      "Epoch 11/300\n",
      "411/411 [==============================] - 0s 551us/sample - loss: 1.6810 - accuracy: 0.6229 - val_loss: 1.4904 - val_accuracy: 0.7273\n",
      "Epoch 12/300\n",
      "411/411 [==============================] - 0s 528us/sample - loss: 1.6188 - accuracy: 0.6788 - val_loss: 1.4754 - val_accuracy: 0.7273\n",
      "Epoch 13/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 1.6104 - accuracy: 0.6180 - val_loss: 1.4601 - val_accuracy: 0.7273\n",
      "Epoch 14/300\n",
      "411/411 [==============================] - 0s 655us/sample - loss: 1.5954 - accuracy: 0.6375 - val_loss: 1.4472 - val_accuracy: 0.7273\n",
      "Epoch 15/300\n",
      "411/411 [==============================] - 0s 570us/sample - loss: 1.5237 - accuracy: 0.6861 - val_loss: 1.4455 - val_accuracy: 0.7273\n",
      "Epoch 16/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 1.5252 - accuracy: 0.6959 - val_loss: 1.4414 - val_accuracy: 0.7273\n",
      "Epoch 17/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 1.5099 - accuracy: 0.6837 - val_loss: 1.4262 - val_accuracy: 0.7273\n",
      "Epoch 18/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 1.4873 - accuracy: 0.6813 - val_loss: 1.4163 - val_accuracy: 0.7273\n",
      "Epoch 19/300\n",
      "411/411 [==============================] - 0s 542us/sample - loss: 1.5043 - accuracy: 0.6764 - val_loss: 1.4009 - val_accuracy: 0.7273\n",
      "Epoch 20/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 1.5088 - accuracy: 0.6910 - val_loss: 1.3850 - val_accuracy: 0.7273\n",
      "Epoch 21/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 1.4229 - accuracy: 0.6837 - val_loss: 1.3744 - val_accuracy: 0.7273\n",
      "Epoch 22/300\n",
      "411/411 [==============================] - 0s 677us/sample - loss: 1.4095 - accuracy: 0.7007 - val_loss: 1.3811 - val_accuracy: 0.7273\n",
      "Epoch 23/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 1.4315 - accuracy: 0.7056 - val_loss: 1.3692 - val_accuracy: 0.7273\n",
      "Epoch 24/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 1.4070 - accuracy: 0.7007 - val_loss: 1.3528 - val_accuracy: 0.7273\n",
      "Epoch 25/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 1.4415 - accuracy: 0.6813 - val_loss: 1.3451 - val_accuracy: 0.7273\n",
      "Epoch 26/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 1.4033 - accuracy: 0.7153 - val_loss: 1.3319 - val_accuracy: 0.7273\n",
      "Epoch 27/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 1.3255 - accuracy: 0.7299 - val_loss: 1.3128 - val_accuracy: 0.7273\n",
      "Epoch 28/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 1.3647 - accuracy: 0.6983 - val_loss: 1.3074 - val_accuracy: 0.7273\n",
      "Epoch 29/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 1.3231 - accuracy: 0.7178 - val_loss: 1.2948 - val_accuracy: 0.7273\n",
      "Epoch 30/300\n",
      "411/411 [==============================] - 0s 532us/sample - loss: 1.3160 - accuracy: 0.7275 - val_loss: 1.2763 - val_accuracy: 0.7273\n",
      "Epoch 31/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 1.3124 - accuracy: 0.6983 - val_loss: 1.2878 - val_accuracy: 0.7273\n",
      "Epoch 32/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 1.3019 - accuracy: 0.7178 - val_loss: 1.2702 - val_accuracy: 0.7273\n",
      "Epoch 33/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 1.2492 - accuracy: 0.7226 - val_loss: 1.2668 - val_accuracy: 0.7273\n",
      "Epoch 34/300\n",
      "411/411 [==============================] - 0s 525us/sample - loss: 1.2892 - accuracy: 0.6886 - val_loss: 1.2481 - val_accuracy: 0.7273\n",
      "Epoch 35/300\n",
      "411/411 [==============================] - 0s 602us/sample - loss: 1.2518 - accuracy: 0.7348 - val_loss: 1.2490 - val_accuracy: 0.7273\n",
      "Epoch 36/300\n",
      "411/411 [==============================] - 0s 556us/sample - loss: 1.2460 - accuracy: 0.7226 - val_loss: 1.2221 - val_accuracy: 0.7273\n",
      "Epoch 37/300\n",
      "411/411 [==============================] - 0s 573us/sample - loss: 1.1981 - accuracy: 0.7445 - val_loss: 1.2140 - val_accuracy: 0.7273\n",
      "Epoch 38/300\n",
      "411/411 [==============================] - 0s 691us/sample - loss: 1.2252 - accuracy: 0.7202 - val_loss: 1.2213 - val_accuracy: 0.7273\n",
      "Epoch 39/300\n",
      "411/411 [==============================] - 0s 553us/sample - loss: 1.2136 - accuracy: 0.7153 - val_loss: 1.1949 - val_accuracy: 0.7273\n",
      "Epoch 40/300\n",
      "411/411 [==============================] - 0s 532us/sample - loss: 1.2040 - accuracy: 0.7348 - val_loss: 1.1736 - val_accuracy: 0.7273\n",
      "Epoch 41/300\n",
      "411/411 [==============================] - 0s 513us/sample - loss: 1.1945 - accuracy: 0.7397 - val_loss: 1.1377 - val_accuracy: 0.7273\n",
      "Epoch 42/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 1.1925 - accuracy: 0.7251 - val_loss: 1.1108 - val_accuracy: 0.7273\n",
      "Epoch 43/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 1.1844 - accuracy: 0.7251 - val_loss: 1.1069 - val_accuracy: 0.7273\n",
      "Epoch 44/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 1.1666 - accuracy: 0.7299 - val_loss: 1.1222 - val_accuracy: 0.7273\n",
      "Epoch 45/300\n",
      "411/411 [==============================] - 0s 514us/sample - loss: 1.1492 - accuracy: 0.7397 - val_loss: 1.1091 - val_accuracy: 0.7273\n",
      "Epoch 46/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 1.1225 - accuracy: 0.7543 - val_loss: 1.0877 - val_accuracy: 0.7273\n",
      "Epoch 47/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 1.1504 - accuracy: 0.7494 - val_loss: 1.0476 - val_accuracy: 0.7273\n",
      "Epoch 48/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 1.1486 - accuracy: 0.7299 - val_loss: 1.0502 - val_accuracy: 0.7273\n",
      "Epoch 49/300\n",
      "411/411 [==============================] - 0s 568us/sample - loss: 1.1253 - accuracy: 0.7445 - val_loss: 1.0413 - val_accuracy: 0.7273\n",
      "Epoch 50/300\n",
      "411/411 [==============================] - 0s 641us/sample - loss: 1.0873 - accuracy: 0.7421 - val_loss: 1.0366 - val_accuracy: 0.7273\n",
      "Epoch 51/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 1.0787 - accuracy: 0.7762 - val_loss: 1.0712 - val_accuracy: 0.7273\n",
      "Epoch 52/300\n",
      "411/411 [==============================] - 0s 556us/sample - loss: 1.1024 - accuracy: 0.7299 - val_loss: 1.0095 - val_accuracy: 0.8182\n",
      "Epoch 53/300\n",
      "411/411 [==============================] - 0s 607us/sample - loss: 1.0728 - accuracy: 0.7640 - val_loss: 0.9906 - val_accuracy: 0.8182\n",
      "Epoch 54/300\n",
      "411/411 [==============================] - 0s 655us/sample - loss: 1.0684 - accuracy: 0.7689 - val_loss: 0.9844 - val_accuracy: 0.8182\n",
      "Epoch 55/300\n",
      "411/411 [==============================] - 0s 558us/sample - loss: 1.0422 - accuracy: 0.7616 - val_loss: 0.9623 - val_accuracy: 0.8182\n",
      "Epoch 56/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 1.0383 - accuracy: 0.7153 - val_loss: 0.9870 - val_accuracy: 0.7727\n",
      "Epoch 57/300\n",
      "411/411 [==============================] - 0s 546us/sample - loss: 1.0122 - accuracy: 0.7737 - val_loss: 0.9225 - val_accuracy: 0.8636\n",
      "Epoch 58/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.9805 - accuracy: 0.7859 - val_loss: 0.9375 - val_accuracy: 0.9091\n",
      "Epoch 59/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 1.0199 - accuracy: 0.7372 - val_loss: 0.9328 - val_accuracy: 0.8636\n",
      "Epoch 60/300\n",
      "411/411 [==============================] - 0s 592us/sample - loss: 1.0213 - accuracy: 0.7518 - val_loss: 0.9149 - val_accuracy: 0.8182\n",
      "Epoch 61/300\n",
      "411/411 [==============================] - 0s 670us/sample - loss: 0.9919 - accuracy: 0.7810 - val_loss: 0.9455 - val_accuracy: 0.8182\n",
      "Epoch 62/300\n",
      "411/411 [==============================] - 0s 541us/sample - loss: 1.0111 - accuracy: 0.7664 - val_loss: 0.8942 - val_accuracy: 0.8182\n",
      "Epoch 63/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.9773 - accuracy: 0.7810 - val_loss: 0.8822 - val_accuracy: 0.8636\n",
      "Epoch 64/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 1.0015 - accuracy: 0.7348 - val_loss: 0.8527 - val_accuracy: 0.8636\n",
      "Epoch 65/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.9513 - accuracy: 0.7908 - val_loss: 0.8375 - val_accuracy: 0.8636\n",
      "Epoch 66/300\n",
      "411/411 [==============================] - 0s 656us/sample - loss: 0.9806 - accuracy: 0.7640 - val_loss: 0.8888 - val_accuracy: 0.8182\n",
      "Epoch 67/300\n",
      "411/411 [==============================] - 0s 590us/sample - loss: 0.9315 - accuracy: 0.7883 - val_loss: 0.8645 - val_accuracy: 0.8636\n",
      "Epoch 68/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.9255 - accuracy: 0.7883 - val_loss: 0.8580 - val_accuracy: 0.8636\n",
      "Epoch 69/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.9850 - accuracy: 0.7421 - val_loss: 0.8181 - val_accuracy: 0.9091\n",
      "Epoch 70/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.9487 - accuracy: 0.7591 - val_loss: 0.9370 - val_accuracy: 0.8636\n",
      "Epoch 71/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.9225 - accuracy: 0.8029 - val_loss: 0.8023 - val_accuracy: 0.8636\n",
      "Epoch 72/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.9160 - accuracy: 0.7810 - val_loss: 0.8655 - val_accuracy: 0.8182\n",
      "Epoch 73/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.9064 - accuracy: 0.8102 - val_loss: 0.8988 - val_accuracy: 0.8182\n",
      "Epoch 74/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.9030 - accuracy: 0.8005 - val_loss: 0.8488 - val_accuracy: 0.8636\n",
      "Epoch 75/300\n",
      "411/411 [==============================] - 0s 670us/sample - loss: 0.9306 - accuracy: 0.7689 - val_loss: 0.7722 - val_accuracy: 0.9091\n",
      "Epoch 76/300\n",
      "411/411 [==============================] - 0s 550us/sample - loss: 0.8863 - accuracy: 0.7956 - val_loss: 0.8300 - val_accuracy: 0.8636\n",
      "Epoch 77/300\n",
      "411/411 [==============================] - 0s 532us/sample - loss: 0.8631 - accuracy: 0.7908 - val_loss: 0.8322 - val_accuracy: 0.8636\n",
      "Epoch 78/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.8630 - accuracy: 0.7908 - val_loss: 0.8440 - val_accuracy: 0.8182\n",
      "Epoch 79/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.8557 - accuracy: 0.8200 - val_loss: 0.8367 - val_accuracy: 0.8182\n",
      "Epoch 80/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.8941 - accuracy: 0.7616 - val_loss: 0.7955 - val_accuracy: 0.8182\n",
      "Epoch 81/300\n",
      "411/411 [==============================] - 0s 548us/sample - loss: 0.8890 - accuracy: 0.7956 - val_loss: 0.7502 - val_accuracy: 0.8636\n",
      "Epoch 82/300\n",
      "411/411 [==============================] - 0s 670us/sample - loss: 0.8283 - accuracy: 0.8200 - val_loss: 0.7147 - val_accuracy: 0.9091\n",
      "Epoch 83/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.8691 - accuracy: 0.7908 - val_loss: 0.7430 - val_accuracy: 0.8636\n",
      "Epoch 84/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.8426 - accuracy: 0.8175 - val_loss: 0.7429 - val_accuracy: 0.8636\n",
      "Epoch 85/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.8793 - accuracy: 0.7859 - val_loss: 0.7396 - val_accuracy: 0.8636\n",
      "Epoch 86/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.8374 - accuracy: 0.7932 - val_loss: 0.7205 - val_accuracy: 0.8636\n",
      "Epoch 87/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.8420 - accuracy: 0.8054 - val_loss: 0.7216 - val_accuracy: 0.8636\n",
      "Epoch 88/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.8275 - accuracy: 0.7908 - val_loss: 0.6888 - val_accuracy: 0.9091\n",
      "Epoch 89/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.8477 - accuracy: 0.7932 - val_loss: 0.7395 - val_accuracy: 0.8636\n",
      "Epoch 90/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.8363 - accuracy: 0.7883 - val_loss: 0.7008 - val_accuracy: 0.8636\n",
      "Epoch 91/300\n",
      "411/411 [==============================] - 0s 667us/sample - loss: 0.8458 - accuracy: 0.7835 - val_loss: 0.7061 - val_accuracy: 0.8636\n",
      "Epoch 92/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.8227 - accuracy: 0.7908 - val_loss: 0.7103 - val_accuracy: 0.8182\n",
      "Epoch 93/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.7915 - accuracy: 0.7762 - val_loss: 0.7000 - val_accuracy: 0.8636\n",
      "Epoch 94/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.8126 - accuracy: 0.8175 - val_loss: 0.6613 - val_accuracy: 0.9091\n",
      "Epoch 95/300\n",
      "411/411 [==============================] - 0s 532us/sample - loss: 0.7831 - accuracy: 0.8054 - val_loss: 0.6753 - val_accuracy: 0.8636\n",
      "Epoch 96/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.7901 - accuracy: 0.8127 - val_loss: 0.6607 - val_accuracy: 0.9091\n",
      "Epoch 97/300\n",
      "411/411 [==============================] - 0s 530us/sample - loss: 0.7602 - accuracy: 0.8321 - val_loss: 0.7294 - val_accuracy: 0.8636\n",
      "Epoch 98/300\n",
      "411/411 [==============================] - 0s 521us/sample - loss: 0.8061 - accuracy: 0.7835 - val_loss: 0.7679 - val_accuracy: 0.8636\n",
      "Epoch 99/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.8023 - accuracy: 0.7908 - val_loss: 0.6853 - val_accuracy: 0.8636\n",
      "Epoch 100/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.7628 - accuracy: 0.8102 - val_loss: 0.6736 - val_accuracy: 0.8636\n",
      "Epoch 101/300\n",
      "411/411 [==============================] - 0s 553us/sample - loss: 0.7711 - accuracy: 0.8054 - val_loss: 0.6606 - val_accuracy: 0.9091\n",
      "Epoch 102/300\n",
      "411/411 [==============================] - 0s 521us/sample - loss: 0.7884 - accuracy: 0.8054 - val_loss: 0.6593 - val_accuracy: 0.9091\n",
      "Epoch 103/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.7551 - accuracy: 0.8248 - val_loss: 0.6625 - val_accuracy: 0.8636\n",
      "Epoch 104/300\n",
      "411/411 [==============================] - 0s 580us/sample - loss: 0.7711 - accuracy: 0.7981 - val_loss: 0.6364 - val_accuracy: 0.9091\n",
      "Epoch 105/300\n",
      "411/411 [==============================] - 0s 630us/sample - loss: 0.7300 - accuracy: 0.8224 - val_loss: 0.6191 - val_accuracy: 0.9091\n",
      "Epoch 106/300\n",
      "411/411 [==============================] - 0s 546us/sample - loss: 0.7641 - accuracy: 0.7981 - val_loss: 0.7333 - val_accuracy: 0.8636\n",
      "Epoch 107/300\n",
      "411/411 [==============================] - 0s 514us/sample - loss: 0.7524 - accuracy: 0.8224 - val_loss: 0.6240 - val_accuracy: 0.8636\n",
      "Epoch 108/300\n",
      "411/411 [==============================] - 0s 526us/sample - loss: 0.7600 - accuracy: 0.7956 - val_loss: 0.6217 - val_accuracy: 0.8182\n",
      "Epoch 109/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.7226 - accuracy: 0.8127 - val_loss: 0.5963 - val_accuracy: 0.9091\n",
      "Epoch 110/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.7288 - accuracy: 0.8078 - val_loss: 0.5671 - val_accuracy: 0.9091\n",
      "Epoch 111/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.7625 - accuracy: 0.8029 - val_loss: 0.6071 - val_accuracy: 0.8636\n",
      "Epoch 112/300\n",
      "411/411 [==============================] - 0s 542us/sample - loss: 0.7255 - accuracy: 0.8102 - val_loss: 0.6094 - val_accuracy: 0.8636\n",
      "Epoch 113/300\n",
      "411/411 [==============================] - 0s 563us/sample - loss: 0.7443 - accuracy: 0.8005 - val_loss: 0.6558 - val_accuracy: 0.8636\n",
      "Epoch 114/300\n",
      "411/411 [==============================] - 0s 655us/sample - loss: 0.7344 - accuracy: 0.7883 - val_loss: 0.5970 - val_accuracy: 0.9091\n",
      "Epoch 115/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.7297 - accuracy: 0.7835 - val_loss: 0.5975 - val_accuracy: 0.9091\n",
      "Epoch 116/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.7104 - accuracy: 0.8321 - val_loss: 0.6228 - val_accuracy: 0.9091\n",
      "Epoch 117/300\n",
      "411/411 [==============================] - 0s 542us/sample - loss: 0.7237 - accuracy: 0.7932 - val_loss: 0.6462 - val_accuracy: 0.8636\n",
      "Epoch 118/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.7299 - accuracy: 0.8078 - val_loss: 0.6368 - val_accuracy: 0.8636\n",
      "Epoch 119/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.7260 - accuracy: 0.8054 - val_loss: 0.6088 - val_accuracy: 0.8636\n",
      "Epoch 120/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.7361 - accuracy: 0.7932 - val_loss: 0.5870 - val_accuracy: 0.9091\n",
      "Epoch 121/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.7154 - accuracy: 0.7786 - val_loss: 0.5889 - val_accuracy: 0.9091\n",
      "Epoch 122/300\n",
      "411/411 [==============================] - 0s 551us/sample - loss: 0.6973 - accuracy: 0.8078 - val_loss: 0.6018 - val_accuracy: 0.9091\n",
      "Epoch 123/300\n",
      "411/411 [==============================] - 0s 548us/sample - loss: 0.7010 - accuracy: 0.8151 - val_loss: 0.6005 - val_accuracy: 0.9091\n",
      "Epoch 124/300\n",
      "411/411 [==============================] - 0s 660us/sample - loss: 0.6683 - accuracy: 0.8175 - val_loss: 0.6372 - val_accuracy: 0.8636\n",
      "Epoch 125/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.7145 - accuracy: 0.8078 - val_loss: 0.5239 - val_accuracy: 0.9091\n",
      "Epoch 126/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.6706 - accuracy: 0.8321 - val_loss: 0.5600 - val_accuracy: 0.9091\n",
      "Epoch 127/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.6981 - accuracy: 0.8175 - val_loss: 0.5586 - val_accuracy: 0.9091\n",
      "Epoch 128/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.7336 - accuracy: 0.7883 - val_loss: 0.5496 - val_accuracy: 0.9091\n",
      "Epoch 129/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.7011 - accuracy: 0.8005 - val_loss: 0.5382 - val_accuracy: 0.9091\n",
      "Epoch 130/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.6535 - accuracy: 0.8321 - val_loss: 0.5889 - val_accuracy: 0.8636\n",
      "Epoch 131/300\n",
      "411/411 [==============================] - 0s 670us/sample - loss: 0.6929 - accuracy: 0.8054 - val_loss: 0.5459 - val_accuracy: 0.9091\n",
      "Epoch 132/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.6558 - accuracy: 0.8175 - val_loss: 0.5245 - val_accuracy: 0.9091\n",
      "Epoch 133/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.6702 - accuracy: 0.8054 - val_loss: 0.5184 - val_accuracy: 0.9091\n",
      "Epoch 134/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.6822 - accuracy: 0.8102 - val_loss: 0.5432 - val_accuracy: 0.9091\n",
      "Epoch 135/300\n",
      "411/411 [==============================] - 0s 541us/sample - loss: 0.6640 - accuracy: 0.8127 - val_loss: 0.5272 - val_accuracy: 0.9091\n",
      "Epoch 136/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.6435 - accuracy: 0.8248 - val_loss: 0.6618 - val_accuracy: 0.8636\n",
      "Epoch 137/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.6651 - accuracy: 0.8273 - val_loss: 0.4894 - val_accuracy: 0.9545\n",
      "Epoch 138/300\n",
      "411/411 [==============================] - 0s 563us/sample - loss: 0.6610 - accuracy: 0.8175 - val_loss: 0.5500 - val_accuracy: 0.9091\n",
      "Epoch 139/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.6504 - accuracy: 0.8102 - val_loss: 0.5063 - val_accuracy: 0.9091\n",
      "Epoch 140/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.6393 - accuracy: 0.8200 - val_loss: 0.6019 - val_accuracy: 0.8636\n",
      "Epoch 141/300\n",
      "411/411 [==============================] - 0s 541us/sample - loss: 0.6289 - accuracy: 0.8273 - val_loss: 0.5156 - val_accuracy: 0.9091\n",
      "Epoch 142/300\n",
      "411/411 [==============================] - 0s 665us/sample - loss: 0.6455 - accuracy: 0.8321 - val_loss: 0.5976 - val_accuracy: 0.8636\n",
      "Epoch 143/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.6666 - accuracy: 0.7810 - val_loss: 0.5475 - val_accuracy: 0.8636\n",
      "Epoch 144/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.6921 - accuracy: 0.8175 - val_loss: 0.5359 - val_accuracy: 0.9091\n",
      "Epoch 145/300\n",
      "411/411 [==============================] - 0s 561us/sample - loss: 0.6822 - accuracy: 0.8078 - val_loss: 0.5264 - val_accuracy: 0.9091\n",
      "Epoch 146/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.6452 - accuracy: 0.8175 - val_loss: 0.5644 - val_accuracy: 0.9091\n",
      "Epoch 147/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.6312 - accuracy: 0.8175 - val_loss: 0.5374 - val_accuracy: 0.8636\n",
      "Epoch 148/300\n",
      "411/411 [==============================] - 0s 633us/sample - loss: 0.6500 - accuracy: 0.8054 - val_loss: 0.4990 - val_accuracy: 0.9091\n",
      "Epoch 149/300\n",
      "411/411 [==============================] - 0s 662us/sample - loss: 0.6440 - accuracy: 0.8005 - val_loss: 0.4903 - val_accuracy: 0.9091\n",
      "Epoch 150/300\n",
      "411/411 [==============================] - 0s 582us/sample - loss: 0.6189 - accuracy: 0.8345 - val_loss: 0.5164 - val_accuracy: 0.8636\n",
      "Epoch 151/300\n",
      "411/411 [==============================] - 0s 578us/sample - loss: 0.6411 - accuracy: 0.8200 - val_loss: 0.4915 - val_accuracy: 0.9091\n",
      "Epoch 152/300\n",
      "411/411 [==============================] - 0s 585us/sample - loss: 0.6229 - accuracy: 0.8151 - val_loss: 0.4868 - val_accuracy: 0.9091\n",
      "Epoch 153/300\n",
      "411/411 [==============================] - 0s 573us/sample - loss: 0.5999 - accuracy: 0.8297 - val_loss: 0.4737 - val_accuracy: 0.9091\n",
      "Epoch 154/300\n",
      "411/411 [==============================] - 0s 607us/sample - loss: 0.6212 - accuracy: 0.8224 - val_loss: 0.5168 - val_accuracy: 0.8636\n",
      "Epoch 155/300\n",
      "411/411 [==============================] - 0s 612us/sample - loss: 0.6498 - accuracy: 0.8151 - val_loss: 0.4758 - val_accuracy: 0.9091\n",
      "Epoch 156/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.6097 - accuracy: 0.8151 - val_loss: 0.5063 - val_accuracy: 0.8636\n",
      "Epoch 157/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.6109 - accuracy: 0.8321 - val_loss: 0.4865 - val_accuracy: 0.9091\n",
      "Epoch 158/300\n",
      "411/411 [==============================] - 0s 517us/sample - loss: 0.6338 - accuracy: 0.8029 - val_loss: 0.4748 - val_accuracy: 0.9091\n",
      "Epoch 159/300\n",
      "411/411 [==============================] - 0s 517us/sample - loss: 0.6276 - accuracy: 0.8224 - val_loss: 0.4829 - val_accuracy: 0.9091\n",
      "Epoch 160/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.6380 - accuracy: 0.8054 - val_loss: 0.4927 - val_accuracy: 0.9091\n",
      "Epoch 161/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.6445 - accuracy: 0.7932 - val_loss: 0.5725 - val_accuracy: 0.8636\n",
      "Epoch 162/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.6166 - accuracy: 0.8054 - val_loss: 0.5302 - val_accuracy: 0.8636\n",
      "Epoch 163/300\n",
      "411/411 [==============================] - 0s 553us/sample - loss: 0.6640 - accuracy: 0.7981 - val_loss: 0.4800 - val_accuracy: 0.9091\n",
      "Epoch 164/300\n",
      "411/411 [==============================] - 0s 582us/sample - loss: 0.5863 - accuracy: 0.8443 - val_loss: 0.4805 - val_accuracy: 0.9091\n",
      "Epoch 165/300\n",
      "411/411 [==============================] - 0s 677us/sample - loss: 0.6200 - accuracy: 0.8175 - val_loss: 0.4888 - val_accuracy: 0.8636\n",
      "Epoch 166/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.6387 - accuracy: 0.8175 - val_loss: 0.4538 - val_accuracy: 0.9091\n",
      "Epoch 167/300\n",
      "411/411 [==============================] - 0s 537us/sample - loss: 0.6182 - accuracy: 0.8054 - val_loss: 0.4650 - val_accuracy: 0.9091\n",
      "Epoch 168/300\n",
      "411/411 [==============================] - 0s 541us/sample - loss: 0.6167 - accuracy: 0.8127 - val_loss: 0.4765 - val_accuracy: 0.9091\n",
      "Epoch 169/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.6447 - accuracy: 0.7835 - val_loss: 0.4571 - val_accuracy: 0.9091\n",
      "Epoch 170/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.5576 - accuracy: 0.8491 - val_loss: 0.5040 - val_accuracy: 0.9091\n",
      "Epoch 171/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.6053 - accuracy: 0.8029 - val_loss: 0.4802 - val_accuracy: 0.9091\n",
      "Epoch 172/300\n",
      "411/411 [==============================] - 0s 679us/sample - loss: 0.6488 - accuracy: 0.8054 - val_loss: 0.4424 - val_accuracy: 0.9091\n",
      "Epoch 173/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.6286 - accuracy: 0.8224 - val_loss: 0.4173 - val_accuracy: 0.9545\n",
      "Epoch 174/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.6117 - accuracy: 0.8248 - val_loss: 0.4178 - val_accuracy: 0.9545\n",
      "Epoch 175/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.6006 - accuracy: 0.8029 - val_loss: 0.4799 - val_accuracy: 0.9091\n",
      "Epoch 176/300\n",
      "411/411 [==============================] - 0s 551us/sample - loss: 0.5749 - accuracy: 0.8248 - val_loss: 0.5301 - val_accuracy: 0.8636\n",
      "Epoch 177/300\n",
      "411/411 [==============================] - 0s 621us/sample - loss: 0.6042 - accuracy: 0.8127 - val_loss: 0.6197 - val_accuracy: 0.7727\n",
      "Epoch 178/300\n",
      "411/411 [==============================] - 0s 641us/sample - loss: 0.6498 - accuracy: 0.7859 - val_loss: 0.4684 - val_accuracy: 0.9091\n",
      "Epoch 179/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.5723 - accuracy: 0.8200 - val_loss: 0.4585 - val_accuracy: 0.9091\n",
      "Epoch 180/300\n",
      "411/411 [==============================] - 0s 512us/sample - loss: 0.6028 - accuracy: 0.8175 - val_loss: 0.5091 - val_accuracy: 0.8636\n",
      "Epoch 181/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.5912 - accuracy: 0.8200 - val_loss: 0.4601 - val_accuracy: 0.9091\n",
      "Epoch 182/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.6322 - accuracy: 0.7956 - val_loss: 0.4667 - val_accuracy: 0.9091\n",
      "Epoch 183/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.5810 - accuracy: 0.8321 - val_loss: 0.4482 - val_accuracy: 0.9091\n",
      "Epoch 184/300\n",
      "411/411 [==============================] - 0s 515us/sample - loss: 0.5672 - accuracy: 0.8297 - val_loss: 0.4353 - val_accuracy: 0.9091\n",
      "Epoch 185/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.5499 - accuracy: 0.8248 - val_loss: 0.5044 - val_accuracy: 0.9091\n",
      "Epoch 186/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.5475 - accuracy: 0.8321 - val_loss: 0.4345 - val_accuracy: 0.9091\n",
      "Epoch 187/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.5398 - accuracy: 0.8370 - val_loss: 0.4310 - val_accuracy: 0.9091\n",
      "Epoch 188/300\n",
      "411/411 [==============================] - 0s 684us/sample - loss: 0.5677 - accuracy: 0.8321 - val_loss: 0.4982 - val_accuracy: 0.9091\n",
      "Epoch 189/300\n",
      "411/411 [==============================] - 0s 553us/sample - loss: 0.6138 - accuracy: 0.7908 - val_loss: 0.4806 - val_accuracy: 0.9091\n",
      "Epoch 190/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.5780 - accuracy: 0.8175 - val_loss: 0.4445 - val_accuracy: 0.9091\n",
      "Epoch 191/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.5829 - accuracy: 0.8078 - val_loss: 0.4604 - val_accuracy: 0.9091\n",
      "Epoch 192/300\n",
      "411/411 [==============================] - 0s 523us/sample - loss: 0.5784 - accuracy: 0.8248 - val_loss: 0.4569 - val_accuracy: 0.9091\n",
      "Epoch 193/300\n",
      "411/411 [==============================] - 0s 565us/sample - loss: 0.5758 - accuracy: 0.8102 - val_loss: 0.4730 - val_accuracy: 0.9091\n",
      "Epoch 194/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.5922 - accuracy: 0.8224 - val_loss: 0.4788 - val_accuracy: 0.8636\n",
      "Epoch 195/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.5805 - accuracy: 0.8248 - val_loss: 0.5236 - val_accuracy: 0.8636\n",
      "Epoch 196/300\n",
      "411/411 [==============================] - 0s 541us/sample - loss: 0.6058 - accuracy: 0.7883 - val_loss: 0.4976 - val_accuracy: 0.8636\n",
      "Epoch 197/300\n",
      "411/411 [==============================] - 0s 665us/sample - loss: 0.5648 - accuracy: 0.8224 - val_loss: 0.5007 - val_accuracy: 0.8636\n",
      "Epoch 198/300\n",
      "411/411 [==============================] - 0s 548us/sample - loss: 0.5638 - accuracy: 0.8127 - val_loss: 0.4511 - val_accuracy: 0.9091\n",
      "Epoch 199/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.5351 - accuracy: 0.8418 - val_loss: 0.4489 - val_accuracy: 0.8636\n",
      "Epoch 200/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.5669 - accuracy: 0.8224 - val_loss: 0.4872 - val_accuracy: 0.8636\n",
      "Epoch 201/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.5573 - accuracy: 0.8200 - val_loss: 0.4377 - val_accuracy: 0.9091\n",
      "Epoch 202/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.5690 - accuracy: 0.8200 - val_loss: 0.5170 - val_accuracy: 0.8636\n",
      "Epoch 203/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.5426 - accuracy: 0.8102 - val_loss: 0.4613 - val_accuracy: 0.9091\n",
      "Epoch 204/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.5361 - accuracy: 0.8321 - val_loss: 0.3941 - val_accuracy: 0.9091\n",
      "Epoch 205/300\n",
      "411/411 [==============================] - 0s 553us/sample - loss: 0.5358 - accuracy: 0.8370 - val_loss: 0.4621 - val_accuracy: 0.9091\n",
      "Epoch 206/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.5743 - accuracy: 0.8151 - val_loss: 0.4526 - val_accuracy: 0.8636\n",
      "Epoch 207/300\n",
      "411/411 [==============================] - 0s 517us/sample - loss: 0.5718 - accuracy: 0.8175 - val_loss: 0.4578 - val_accuracy: 0.8636\n",
      "Epoch 208/300\n",
      "411/411 [==============================] - 0s 612us/sample - loss: 0.5613 - accuracy: 0.8151 - val_loss: 0.4635 - val_accuracy: 0.8636\n",
      "Epoch 209/300\n",
      "411/411 [==============================] - 0s 612us/sample - loss: 0.5442 - accuracy: 0.8273 - val_loss: 0.4299 - val_accuracy: 0.9091\n",
      "Epoch 210/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.5464 - accuracy: 0.8248 - val_loss: 0.4173 - val_accuracy: 0.9091\n",
      "Epoch 211/300\n",
      "411/411 [==============================] - 0s 515us/sample - loss: 0.5590 - accuracy: 0.8370 - val_loss: 0.4299 - val_accuracy: 0.9091\n",
      "Epoch 212/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.5585 - accuracy: 0.8175 - val_loss: 0.4014 - val_accuracy: 0.9091\n",
      "Epoch 213/300\n",
      "411/411 [==============================] - 0s 706us/sample - loss: 0.5470 - accuracy: 0.8102 - val_loss: 0.4790 - val_accuracy: 0.8636\n",
      "Epoch 214/300\n",
      "411/411 [==============================] - 0s 561us/sample - loss: 0.5419 - accuracy: 0.8418 - val_loss: 0.4424 - val_accuracy: 0.8636\n",
      "Epoch 215/300\n",
      "411/411 [==============================] - 0s 524us/sample - loss: 0.5484 - accuracy: 0.8248 - val_loss: 0.4768 - val_accuracy: 0.8636\n",
      "Epoch 216/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.5296 - accuracy: 0.8200 - val_loss: 0.3839 - val_accuracy: 0.9091\n",
      "Epoch 217/300\n",
      "411/411 [==============================] - 0s 546us/sample - loss: 0.4955 - accuracy: 0.8540 - val_loss: 0.4936 - val_accuracy: 0.9091\n",
      "Epoch 218/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.5269 - accuracy: 0.8394 - val_loss: 0.3545 - val_accuracy: 0.9545\n",
      "Epoch 219/300\n",
      "411/411 [==============================] - 0s 556us/sample - loss: 0.5221 - accuracy: 0.8418 - val_loss: 0.4427 - val_accuracy: 0.9091\n",
      "Epoch 220/300\n",
      "411/411 [==============================] - 0s 507us/sample - loss: 0.4911 - accuracy: 0.8540 - val_loss: 0.3907 - val_accuracy: 0.9091\n",
      "Epoch 221/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.4968 - accuracy: 0.8345 - val_loss: 0.4398 - val_accuracy: 0.9091\n",
      "Epoch 222/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.5204 - accuracy: 0.8321 - val_loss: 0.3817 - val_accuracy: 0.9091\n",
      "Epoch 223/300\n",
      "411/411 [==============================] - 0s 580us/sample - loss: 0.5018 - accuracy: 0.8370 - val_loss: 0.4441 - val_accuracy: 0.9091\n",
      "Epoch 224/300\n",
      "411/411 [==============================] - 0s 648us/sample - loss: 0.5472 - accuracy: 0.8394 - val_loss: 0.4433 - val_accuracy: 0.9091\n",
      "Epoch 225/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.5641 - accuracy: 0.8078 - val_loss: 0.5635 - val_accuracy: 0.8636\n",
      "Epoch 226/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.5210 - accuracy: 0.8394 - val_loss: 0.4329 - val_accuracy: 0.9091\n",
      "Epoch 227/300\n",
      "411/411 [==============================] - 0s 521us/sample - loss: 0.4985 - accuracy: 0.8297 - val_loss: 0.4202 - val_accuracy: 0.9091\n",
      "Epoch 228/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.5273 - accuracy: 0.8394 - val_loss: 0.3972 - val_accuracy: 0.9091\n",
      "Epoch 229/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.5420 - accuracy: 0.8200 - val_loss: 0.4263 - val_accuracy: 0.9091\n",
      "Epoch 230/300\n",
      "411/411 [==============================] - 0s 597us/sample - loss: 0.5444 - accuracy: 0.8248 - val_loss: 0.5243 - val_accuracy: 0.8182\n",
      "Epoch 231/300\n",
      "411/411 [==============================] - 0s 609us/sample - loss: 0.5863 - accuracy: 0.8127 - val_loss: 0.4428 - val_accuracy: 0.8636\n",
      "Epoch 232/300\n",
      "411/411 [==============================] - 0s 561us/sample - loss: 0.5561 - accuracy: 0.8175 - val_loss: 0.4261 - val_accuracy: 0.9091\n",
      "Epoch 233/300\n",
      "411/411 [==============================] - 0s 587us/sample - loss: 0.5019 - accuracy: 0.8467 - val_loss: 0.4291 - val_accuracy: 0.9091\n",
      "Epoch 234/300\n",
      "411/411 [==============================] - 0s 680us/sample - loss: 0.5387 - accuracy: 0.8297 - val_loss: 0.3837 - val_accuracy: 0.9091\n",
      "Epoch 235/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.5244 - accuracy: 0.8200 - val_loss: 0.3794 - val_accuracy: 0.9091\n",
      "Epoch 236/300\n",
      "411/411 [==============================] - 0s 532us/sample - loss: 0.5109 - accuracy: 0.8370 - val_loss: 0.4212 - val_accuracy: 0.9091\n",
      "Epoch 237/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.5283 - accuracy: 0.8200 - val_loss: 0.4022 - val_accuracy: 0.9091\n",
      "Epoch 238/300\n",
      "411/411 [==============================] - 0s 553us/sample - loss: 0.5273 - accuracy: 0.8394 - val_loss: 0.4174 - val_accuracy: 0.9091\n",
      "Epoch 239/300\n",
      "411/411 [==============================] - 0s 678us/sample - loss: 0.5360 - accuracy: 0.8297 - val_loss: 0.4074 - val_accuracy: 0.9091\n",
      "Epoch 240/300\n",
      "411/411 [==============================] - 0s 585us/sample - loss: 0.5102 - accuracy: 0.8467 - val_loss: 0.3980 - val_accuracy: 0.9091\n",
      "Epoch 241/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.4868 - accuracy: 0.8589 - val_loss: 0.3925 - val_accuracy: 0.9091\n",
      "Epoch 242/300\n",
      "411/411 [==============================] - 0s 506us/sample - loss: 0.4941 - accuracy: 0.8491 - val_loss: 0.4158 - val_accuracy: 0.9091\n",
      "Epoch 243/300\n",
      "411/411 [==============================] - 0s 622us/sample - loss: 0.4835 - accuracy: 0.8467 - val_loss: 0.4039 - val_accuracy: 0.9091\n",
      "Epoch 244/300\n",
      "411/411 [==============================] - 0s 636us/sample - loss: 0.4887 - accuracy: 0.8370 - val_loss: 0.3979 - val_accuracy: 0.9091\n",
      "Epoch 245/300\n",
      "411/411 [==============================] - 0s 551us/sample - loss: 0.5007 - accuracy: 0.8491 - val_loss: 0.3376 - val_accuracy: 0.9545\n",
      "Epoch 246/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.4774 - accuracy: 0.8637 - val_loss: 0.4178 - val_accuracy: 0.9091\n",
      "Epoch 247/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.4918 - accuracy: 0.8321 - val_loss: 0.4155 - val_accuracy: 0.9091\n",
      "Epoch 248/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.4874 - accuracy: 0.8491 - val_loss: 0.4908 - val_accuracy: 0.8636\n",
      "Epoch 249/300\n",
      "411/411 [==============================] - 0s 529us/sample - loss: 0.5741 - accuracy: 0.7859 - val_loss: 0.3966 - val_accuracy: 0.9091\n",
      "Epoch 250/300\n",
      "411/411 [==============================] - 0s 512us/sample - loss: 0.5119 - accuracy: 0.8200 - val_loss: 0.4111 - val_accuracy: 0.9091\n",
      "Epoch 251/300\n",
      "411/411 [==============================] - 0s 558us/sample - loss: 0.5148 - accuracy: 0.8345 - val_loss: 0.4250 - val_accuracy: 0.9091\n",
      "Epoch 252/300\n",
      "411/411 [==============================] - 0s 694us/sample - loss: 0.5361 - accuracy: 0.8175 - val_loss: 0.5140 - val_accuracy: 0.9091\n",
      "Epoch 253/300\n",
      "411/411 [==============================] - 0s 563us/sample - loss: 0.5551 - accuracy: 0.8078 - val_loss: 0.4125 - val_accuracy: 0.8636\n",
      "Epoch 254/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.5177 - accuracy: 0.8516 - val_loss: 0.3755 - val_accuracy: 0.9091\n",
      "Epoch 255/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.5342 - accuracy: 0.8491 - val_loss: 0.4312 - val_accuracy: 0.9091\n",
      "Epoch 256/300\n",
      "411/411 [==============================] - 0s 556us/sample - loss: 0.5376 - accuracy: 0.8370 - val_loss: 0.3715 - val_accuracy: 0.9091\n",
      "Epoch 257/300\n",
      "411/411 [==============================] - 0s 585us/sample - loss: 0.5085 - accuracy: 0.8467 - val_loss: 0.4433 - val_accuracy: 0.8636\n",
      "Epoch 258/300\n",
      "411/411 [==============================] - 0s 646us/sample - loss: 0.5205 - accuracy: 0.8443 - val_loss: 0.4295 - val_accuracy: 0.8636\n",
      "Epoch 259/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.5121 - accuracy: 0.8491 - val_loss: 0.4923 - val_accuracy: 0.8636\n",
      "Epoch 260/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.4789 - accuracy: 0.8516 - val_loss: 0.3952 - val_accuracy: 0.9091\n",
      "Epoch 261/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.4964 - accuracy: 0.8491 - val_loss: 0.3922 - val_accuracy: 0.9091\n",
      "Epoch 262/300\n",
      "411/411 [==============================] - 0s 523us/sample - loss: 0.4973 - accuracy: 0.8418 - val_loss: 0.4157 - val_accuracy: 0.9091\n",
      "Epoch 263/300\n",
      "411/411 [==============================] - 0s 609us/sample - loss: 0.5168 - accuracy: 0.8418 - val_loss: 0.4788 - val_accuracy: 0.8636\n",
      "Epoch 264/300\n",
      "411/411 [==============================] - 0s 646us/sample - loss: 0.4909 - accuracy: 0.8394 - val_loss: 0.4820 - val_accuracy: 0.8636\n",
      "Epoch 265/300\n",
      "411/411 [==============================] - 0s 580us/sample - loss: 0.4712 - accuracy: 0.8564 - val_loss: 0.3670 - val_accuracy: 0.9091\n",
      "Epoch 266/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.4776 - accuracy: 0.8540 - val_loss: 0.4164 - val_accuracy: 0.9091\n",
      "Epoch 267/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.4887 - accuracy: 0.8467 - val_loss: 0.4312 - val_accuracy: 0.9091\n",
      "Epoch 268/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.4963 - accuracy: 0.8491 - val_loss: 0.4610 - val_accuracy: 0.8636\n",
      "Epoch 269/300\n",
      "411/411 [==============================] - 0s 544us/sample - loss: 0.5192 - accuracy: 0.8224 - val_loss: 0.5094 - val_accuracy: 0.8636\n",
      "Epoch 270/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.5217 - accuracy: 0.8273 - val_loss: 0.3821 - val_accuracy: 0.9091\n",
      "Epoch 271/300\n",
      "411/411 [==============================] - 0s 680us/sample - loss: 0.5765 - accuracy: 0.8151 - val_loss: 0.4113 - val_accuracy: 0.8636\n",
      "Epoch 272/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.5361 - accuracy: 0.8054 - val_loss: 0.3682 - val_accuracy: 0.9091\n",
      "Epoch 273/300\n",
      "411/411 [==============================] - 0s 546us/sample - loss: 0.5059 - accuracy: 0.8273 - val_loss: 0.3945 - val_accuracy: 0.8636\n",
      "Epoch 274/300\n",
      "411/411 [==============================] - 0s 534us/sample - loss: 0.4886 - accuracy: 0.8467 - val_loss: 0.3666 - val_accuracy: 0.9091\n",
      "Epoch 275/300\n",
      "411/411 [==============================] - 0s 510us/sample - loss: 0.4806 - accuracy: 0.8589 - val_loss: 0.3776 - val_accuracy: 0.9091\n",
      "Epoch 276/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.4468 - accuracy: 0.8808 - val_loss: 0.4299 - val_accuracy: 0.9091\n",
      "Epoch 277/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.4898 - accuracy: 0.8491 - val_loss: 0.3460 - val_accuracy: 0.9091\n",
      "Epoch 278/300\n",
      "411/411 [==============================] - 0s 556us/sample - loss: 0.4831 - accuracy: 0.8443 - val_loss: 0.4557 - val_accuracy: 0.8636\n",
      "Epoch 279/300\n",
      "411/411 [==============================] - 0s 536us/sample - loss: 0.4506 - accuracy: 0.8491 - val_loss: 0.3746 - val_accuracy: 0.9091\n",
      "Epoch 280/300\n",
      "411/411 [==============================] - 0s 670us/sample - loss: 0.4546 - accuracy: 0.8759 - val_loss: 0.3610 - val_accuracy: 0.9091\n",
      "Epoch 281/300\n",
      "411/411 [==============================] - 0s 599us/sample - loss: 0.4717 - accuracy: 0.8467 - val_loss: 0.4338 - val_accuracy: 0.9091\n",
      "Epoch 282/300\n",
      "411/411 [==============================] - 0s 527us/sample - loss: 0.4616 - accuracy: 0.8613 - val_loss: 0.4340 - val_accuracy: 0.9091\n",
      "Epoch 283/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.4546 - accuracy: 0.8589 - val_loss: 0.3330 - val_accuracy: 0.9091\n",
      "Epoch 284/300\n",
      "411/411 [==============================] - 0s 517us/sample - loss: 0.4870 - accuracy: 0.8418 - val_loss: 0.3473 - val_accuracy: 0.9091\n",
      "Epoch 285/300\n",
      "411/411 [==============================] - 0s 573us/sample - loss: 0.4837 - accuracy: 0.8443 - val_loss: 0.4034 - val_accuracy: 0.9091\n",
      "Epoch 286/300\n",
      "411/411 [==============================] - 0s 670us/sample - loss: 0.4823 - accuracy: 0.8467 - val_loss: 0.3995 - val_accuracy: 0.9091\n",
      "Epoch 287/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.4621 - accuracy: 0.8637 - val_loss: 0.3976 - val_accuracy: 0.9091\n",
      "Epoch 288/300\n",
      "411/411 [==============================] - 0s 551us/sample - loss: 0.4589 - accuracy: 0.8589 - val_loss: 0.4267 - val_accuracy: 0.9091\n",
      "Epoch 289/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.4647 - accuracy: 0.8540 - val_loss: 0.4315 - val_accuracy: 0.9091\n",
      "Epoch 290/300\n",
      "411/411 [==============================] - 0s 531us/sample - loss: 0.4710 - accuracy: 0.8418 - val_loss: 0.5533 - val_accuracy: 0.8636\n",
      "Epoch 291/300\n",
      "411/411 [==============================] - 0s 548us/sample - loss: 0.4733 - accuracy: 0.8491 - val_loss: 0.3819 - val_accuracy: 0.9091\n",
      "Epoch 292/300\n",
      "411/411 [==============================] - 0s 650us/sample - loss: 0.4937 - accuracy: 0.8345 - val_loss: 0.3517 - val_accuracy: 0.9091\n",
      "Epoch 293/300\n",
      "411/411 [==============================] - 0s 641us/sample - loss: 0.4768 - accuracy: 0.8516 - val_loss: 0.3511 - val_accuracy: 0.9091\n",
      "Epoch 294/300\n",
      "411/411 [==============================] - 0s 578us/sample - loss: 0.4617 - accuracy: 0.8491 - val_loss: 0.4120 - val_accuracy: 0.9091\n",
      "Epoch 295/300\n",
      "411/411 [==============================] - 0s 556us/sample - loss: 0.4840 - accuracy: 0.8516 - val_loss: 0.3720 - val_accuracy: 0.9091\n",
      "Epoch 296/300\n",
      "411/411 [==============================] - 0s 546us/sample - loss: 0.4480 - accuracy: 0.8564 - val_loss: 0.3935 - val_accuracy: 0.9091\n",
      "Epoch 297/300\n",
      "411/411 [==============================] - 0s 522us/sample - loss: 0.4852 - accuracy: 0.8491 - val_loss: 0.4051 - val_accuracy: 0.9091\n",
      "Epoch 298/300\n",
      "411/411 [==============================] - 0s 519us/sample - loss: 0.4678 - accuracy: 0.8345 - val_loss: 0.3569 - val_accuracy: 0.9091\n",
      "Epoch 299/300\n",
      "411/411 [==============================] - 0s 539us/sample - loss: 0.4570 - accuracy: 0.8443 - val_loss: 0.3474 - val_accuracy: 0.9091\n",
      "Epoch 300/300\n",
      "411/411 [==============================] - 0s 706us/sample - loss: 0.4727 - accuracy: 0.8516 - val_loss: 0.3742 - val_accuracy: 0.9091\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "109/109 [==============================] - 0s 2ms/sample - loss: 0.4312 - accuracy: 0.8440\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFICATION ###\n",
    "# We use the reduced dataset to train a classifier and compare it against the same classifier trained with the original dataset.\n",
    "\n",
    "# One hot encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "OH_encoder = LabelEncoder()\n",
    "OH_y_train = pd.DataFrame(OH_encoder.fit_transform(y_train))\n",
    "OH_y_test = pd.DataFrame(OH_encoder.transform(y_test))\n",
    "y_train_oh = keras.utils.to_categorical(OH_y_train)\n",
    "y_test_oh = keras.utils.to_categorical(OH_y_test)\n",
    "\n",
    "## Definition of the best classifier obtained previously (CTG_dataset_classification)\n",
    "def build_best_model(dropout: int, l1: int, l2: int, input_shape: int):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(2000, activation=tf.nn.relu ,kernel_regularizer=keras.regularizers.l1_l2(l1,l2), input_shape=(input_shape,)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(200,activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l1_l2(l1,l2)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(20,activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l1_l2(l1,l2)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(2,activation=tf.nn.softmax)\n",
    "      ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fit best model with dimensionality reduction data\n",
    "model_ae = build_best_model(0.5,0.0001, 0.0001, X_train_encoded.shape[1])\n",
    "history_ae = model_ae.fit(X_train_encoded, y_train_oh, epochs=300,\n",
    "                    validation_split = 0.05, verbose=1, callbacks=[], shuffle=False)\n",
    "hist_ae = pd.DataFrame(history_ae.history)\n",
    "\n",
    "test_loss, test_acc = model_ae.evaluate(X_test_encoded, y_test_oh)\n",
    "\n",
    "# Fit best model with concatenated data\n",
    "#model = build_best_model(0.5,0.0001,0.0001, X_train_norm.shape[1])\n",
    "#history = model.fit(X_train, y_train_oh, epochs=150,\n",
    "#                    validation_split = 0.1, verbose=1, callbacks=[early_stop])\n",
    "#hist = pd.DataFrame(history.history)\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(X_test, y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CLUSTERING ###\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "ae_cluster_labels = kmeans.fit_predict(X_train_encoded)\n",
    "ae_silhouette_avg = silhouette_score(X_train_ae, ae_cluster_labels)\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(X_train)\n",
    "silhouette_avg = silhouette_score(X_train, cluster_labels)\n",
    "\n",
    "print(f\"AE silhoutte score: {ae_silhouette_avg}\")\n",
    "print(f\"Original silhoutte score: {silhouette_avg}\")\n",
    "\n",
    "### PLOT SILOHUETTE SCORE FOR CLUSTERS\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(18, 7)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1 but in this example all\n",
    "# lie within [-0.1, 1]\n",
    "ax1.set_xlim([-0.1, 1])\n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, len(X_train_ae) + (n_clusters + 1) * 10])\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X_train_ae, ae_cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                      0, ith_cluster_silhouette_values,\n",
    "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax1.axvline(x=ae_silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X_train, cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax2.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                      0, ith_cluster_silhouette_values,\n",
    "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax2.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax2.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax2.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax2.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax2.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax2.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 20502)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-52c6c4988414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_encoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
